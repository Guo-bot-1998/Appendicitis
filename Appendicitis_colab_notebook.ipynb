{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guo-bot-1998/Appendicitis/blob/master/Appendicitis_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coYrDLBnCbwQ"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4kBLfXWRg7N"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n",
        "!pip install tqdm\n",
        "!pip install kora\n",
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ4w8h2LkY0g"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import timm\n",
        "import gdown\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from kora.xattr import get_id\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "from tqdm.asyncio import tqdm\n",
        "nest_asyncio.apply()\n",
        "loop = asyncio.get_event_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 存取"
      ],
      "metadata": {
        "id": "Oc3DYvY3WsL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_disk(driveroot, targetdir):\n",
        "  \"\"\"從雲端資料夾targetdir下載檔案到colab root底下同名資料夾\n",
        "  只下載沒有下載的檔案.\n",
        "\n",
        "  driveroot為雲端檔案資料夾root\"\"\"\n",
        "\n",
        "  if not os.path.exists('/content/' + targetdir):\n",
        "    os.makedirs(targetdir)\n",
        "    print(f\"making {targetdir} dir...\")\n",
        "\n",
        "  files_on_drive = os.listdir(driveroot+'/'+targetdir)\n",
        "  files_in_localdir = os.listdir(f\"/content/{targetdir}\")\n",
        "  files_to_download = [f for f in files_on_drive if f not in files_in_localdir]\n",
        "  files = [(filename, get_id(f'{driveroot}/{targetdir}/{filename}')) for filename in files_to_download]\n",
        "\n",
        "\n",
        "  for idx, (name, file_id) in enumerate(files):\n",
        "\n",
        "    if not file_id:\n",
        "      print(f\"未找到文件: {name}\")\n",
        "      continue\n",
        "\n",
        "    # 下载文件\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "      status, done = downloader.next_chunk()\n",
        "      # print(f\"下载 {name} 进度: {int(status.progress() * 100)}%\")\n",
        "\n",
        "    # 将文件内容写入本地文件\n",
        "    local_file_path = os.path.join('/content', targetdir, name)\n",
        "\n",
        "    with open(local_file_path, 'wb') as f:\n",
        "      fh.seek(0)\n",
        "      f.write(fh.read())\n",
        "\n",
        "    print(f\"{idx+1}/{len(files)}: {name} downloaded\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "async def download_file(session, name, file_id, file_path, token):\n",
        "  if not file_id:\n",
        "    print(f\"未找到文件: {name}\")\n",
        "    return\n",
        "\n",
        "  download_url = f\"https://www.googleapis.com/drive/v3/files/{file_id}?alt=media\"\n",
        "  headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "  async with session.get(download_url, headers=headers) as response:\n",
        "    if response.status == 200:\n",
        "      with open(file_path, 'wb') as f:\n",
        "        while True:\n",
        "          chunk = await response.content.read(1024)\n",
        "          if not chunk:\n",
        "            break\n",
        "          f.write(chunk)\n",
        "      # print(f\"Downloaded: {name}...\")\n",
        "    else:\n",
        "      print(f\"Failed: {name}!! status: {response.status}\")\n",
        "\n",
        "\n",
        "# 定义异步获取数据的主函数\n",
        "async def get_data_disk_async(driveroot, targetdir, token):\n",
        "  # 生成文件夹的绝对路径\n",
        "  drive_folder_path = os.path.join(driveroot, targetdir)\n",
        "  local_folder_path = os.path.join('/content', targetdir)\n",
        "\n",
        "  if not os.path.exists(local_folder_path):\n",
        "    os.makedirs(local_folder_path)\n",
        "    print(f\"making {targetdir} dir...\")\n",
        "\n",
        "  # 获取文件列表\n",
        "  files_on_drive = os.listdir(drive_folder_path)\n",
        "  files_in_localdir = os.listdir(local_folder_path)\n",
        "  files_to_download = [f for f in files_on_drive if f not in files_in_localdir]\n",
        "  files = [(filename, get_id(os.path.join(drive_folder_path, filename))) for filename in files_to_download]\n",
        "\n",
        "  async with aiohttp.ClientSession() as session:\n",
        "    tasks = []\n",
        "    for idx, (name, file_id) in enumerate(files):\n",
        "      local_file_path = os.path.join(local_folder_path, name)\n",
        "      task = asyncio.create_task(download_file(session, name, file_id, local_file_path, token))\n",
        "      tasks.append(task)\n",
        "\n",
        "    # 使用 tqdm 包装任务列表以显示进度条\n",
        "    for f in tqdm.as_completed(tasks, total=len(tasks)):\n",
        "      await f\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_data2(traindir, shift=0, termi=10, dropFalse=0.85, dropPos=0)\\\n",
        " -> tuple[torch.tensor, torch.tensor]:\n",
        "  \"\"\"Reading data in \"traindir\" and process them.\n",
        "  Taking care of memory efficiency\"\n",
        "\n",
        "  Args:\n",
        "    shift: 從第幾個開始讀取\n",
        "    termi: 讀取幾個\n",
        "    dropFalse: 被刪除的有闌尾炎的張數比例\n",
        "    dropPos: 被刪除的有闌尾炎的張數比例\n",
        "\n",
        "  Returns:\n",
        "    images: 输出的图像数据\n",
        "    labels: 输出的标签数据\n",
        "  \"\"\"\n",
        "  print(f\"reading {traindir=}...\")\n",
        "  labels_ = read_label(labelpath)\n",
        "\n",
        "  if labels_.index.name != 'id':\n",
        "    labels_.set_index('id', inplace=True)\n",
        "\n",
        "  #裁減過的圖片放在Cropped_[範圍]的資料夾下,用路徑名稱判斷資料使否裁減過\n",
        "  cropmatch = re.search(r'Cropped', traindir)\n",
        "  if not cropmatch:\n",
        "    print(\"讀取未切片的資料夾\")\n",
        "    xlim, ylim, zlim = [0,512], [0,512], [0,None]\n",
        "  else:\n",
        "    #範圍 = xstart-xend_ystart-yend_zstart-zend\n",
        "    ismatch = re.search(r'(\\d+-\\d+)_(\\d+-\\d+)_(\\d+-\\d+)', traindir)\n",
        "    if ismatch:\n",
        "      dimens = ismatch.group(0)\n",
        "      labelidx = dimens.split('_')\n",
        "      xlim = [int(idx) for idx in labelidx[0].split('-')]\n",
        "      ylim = [int(idx) for idx in labelidx[1].split('-')]\n",
        "      zlim = [int(idx) for idx in labelidx[2].split('-')]\n",
        "      print(f\"{xlim=}\\n{ylim=}\\n{zlim=}\")\n",
        "      if xlim[0] >= xlim[1] or ylim[0] >= ylim[1] or zlim[0] >= zlim[1]:\n",
        "        print(\"切片格式不正确\")\n",
        "        return\n",
        "    else:\n",
        "      print(\"切片格式不正确\")\n",
        "      return\n",
        "\n",
        "\n",
        "  filelist = os.listdir(traindir)\n",
        "  filelist = filelist[shift:shift+termi]\n",
        "\n",
        "  # preallocated\n",
        "  if not cropmatch:\n",
        "    selecteds = [labels_.loc[labels_.index.str.startswith(afile.strip('.nii.gz')+'_')] for afile in filelist]\n",
        "    numcuts = [len(selected) for selected in selecteds]\n",
        "    numcuts = sum(numcuts)\n",
        "  else:\n",
        "    numcuts = (zlim[1]-zlim[0]) * len(filelist)\n",
        "  images = torch.zeros(numcuts,1, ylim[-1]-ylim[0],xlim[-1]-xlim[0])\n",
        "  labels = -torch.ones(numcuts)\n",
        "  nimgs = 0\n",
        "  nprocess = 0\n",
        "\n",
        "  for key in filelist:\n",
        "    file_path = os.path.join(traindir, key)\n",
        "    key = key.strip('.nii.gz')\n",
        "    scan  = labels_.loc[labels_.index.str.startswith(key+'_')]\n",
        "    value = nib.load(file_path).get_fdata()\n",
        "\n",
        "    label_t = torch.tensor(scan['label'][zlim[0]:zlim[-1]])\n",
        "    image_t = torch.from_numpy(value).float().permute(2, 0, 1).unsqueeze(1)\n",
        "\n",
        "    #有幾筆不是512x512\n",
        "    if (not cropmatch) and (image_t.shape[2] != 512 or image_t.shape[3] != 512):\n",
        "      image_t = cropping(image_t)\n",
        "\n",
        "    image_t, label_t = remove_false_images(image_t, label_t, dropFalse)\n",
        "    image_t, label_t = remove_positive_images(image_t, label_t, dropPos)\n",
        "\n",
        "    n_new = len(label_t)\n",
        "\n",
        "    images[nimgs:nimgs+n_new] = image_t\n",
        "    labels[nimgs:nimgs+n_new] = label_t\n",
        "    nimgs += len(label_t)\n",
        "    print(f\"Process {nprocess}: {key} finished...\")\n",
        "    nprocess += 1\n",
        "\n",
        "\n",
        "  #after drop\n",
        "  counts = count_zero(images)\n",
        "  images = torch.asarray(images[:len(images)-counts])\n",
        "  labels = labels[labels != -1]\n",
        "\n",
        "  return images, labels.float()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_test(testdir,shift=0,termi=10):\n",
        "  \"\"\"Reading data in \"testdir\"\n",
        "  not memory efficient\n",
        "  Args:\n",
        "    shift: 從第幾個開始讀取\n",
        "    termi: 讀取幾個\n",
        "  Returns:\n",
        "    images: 输出的图像数据\n",
        "    scans_info: 每個scan的檔名和切片數(原本nii檔所包含的數量)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  filelist = os.listdir(testdir)\n",
        "  filelist = filelist[shift:shift+termi]\n",
        "  scans_info = []\n",
        "\n",
        "  images = []\n",
        "  nprocess = 0\n",
        "\n",
        "  for key in filelist:\n",
        "    file_path = os.path.join(testdir, key)\n",
        "    key = key.strip('.nii.gz')\n",
        "    value = nib.load(file_path).get_fdata()\n",
        "    scans_info.append((key, value.shape[2]))\n",
        "\n",
        "    image_t = torch.from_numpy(value).float().permute(2, 0, 1).unsqueeze(1)\n",
        "\n",
        "    images.append(image_t)\n",
        "    print(f\"{nprocess}: Read test {key} finished...\")\n",
        "    nprocess += 1\n",
        "\n",
        "  images = torch.cat(images,dim=0)\n",
        "  print(f\"read {len(scans_info)} scans\")\n",
        "\n",
        "  return images, scans_info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model, modelname=\"\", dirname=\"\", root=os.path.realpath(\"/content/drive/MyDrive/AOCR2024/params\")):\n",
        "\n",
        "  if not model:\n",
        "    print(\"not given model\")\n",
        "    return\n",
        "\n",
        "  if not os.path.exists(root):\n",
        "    print(f\"{root} not exists!\")\n",
        "    return\n",
        "\n",
        "  print(f\"model will be saved as {root}/{dirname}/{modelname}\")\n",
        "\n",
        "  if not modelname:\n",
        "    modelname = input(\"請輸入模型儲存的檔名:\")\n",
        "  if not dirname:\n",
        "    dirname = input(\"請輸入模型儲存的資料夾:\")\n",
        "\n",
        "  filename = f\"{root}/{dirname}/{modelname}\"\n",
        "\n",
        "  if not os.path.exists(os.path.dirname(filename)):\n",
        "    os.mkdir(os.path.dirname(filename))\n",
        "\n",
        "  if os.path.isfile(filename+'.pth'):\n",
        "      print(f\"{filename}.pth exist.\")\n",
        "  else:\n",
        "      torch.save(model.state_dict(), f'{filename}.pth')\n",
        "\n",
        "  return filename\n",
        "\n",
        "\n",
        "\n",
        "def read_label(excel_path) -> pd.DataFrame:\n",
        "  \"\"\"Reads a csv file containing ground-truth.\n",
        "    The csv file should have two columns: 'id' and 'label'.\n",
        "  \"\"\"\n",
        "  with open(excel_path, 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "    df.set_index('id', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def write_error(logpath, e, nprocess):\n",
        "  # with open(logpath, 'a') as f:\n",
        "    # f.write(f\"{nprocess:} error occured: {e}\\n\") ##! may stuck the program!!\n",
        "  print(f\"{nprocess:} error occured: {e}\\n\")\n",
        "\n",
        "\n",
        "def read_submission(excel_path) -> pd.DataFrame:\n",
        "  \"\"\"Reads a csv file containing submission file.\n",
        "    The csv file should follow the format given by Kaggle.\n",
        "  \"\"\"\n",
        "  with open(excel_path, 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "    df.set_index('id', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KLs_ItZEW0F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 列印結果"
      ],
      "metadata": {
        "id": "Fnggb7x7X8Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion(guess, truth):\n",
        "  right = guess == truth\n",
        "  wrong = np.logical_not(right)\n",
        "  TP = np.sum(np.logical_and(right, truth == np.ones(right.shape)))\n",
        "  TN = np.sum(np.logical_and(right, truth == np.zeros(right.shape)))\n",
        "  FN = np.sum(np.logical_and(wrong, truth == np.ones(wrong.shape)))\n",
        "  FP = np.sum(np.logical_and(wrong, truth == np.zeros(wrong.shape)))\n",
        "  return (TP,FP,FN,TN)\n",
        "\n",
        "\n",
        "\n",
        "def print_results(prediction, labels):\n",
        "  if torch.is_tensor(prediction):\n",
        "    prediction = prediction.cpu().numpy()\n",
        "  if torch.is_tensor(labels):\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "  TP,FP,FN,TN = get_confusion(prediction, labels)\n",
        "  print(f\"\\n\\\n",
        "      真實值\\n\\\n",
        "  預  +-----+-----+\\n\\\n",
        "  測| TP: {TP}| FP: {FP}|\\n\\\n",
        "  值| FN: {FN}| TN: {TN}|\\n\\\n",
        "      +-----+-----+ \\n\")\n",
        "\n",
        "  recall = TP/(TP + FN)\n",
        "  precision = TP/(TP + FP)\n",
        "  recall = 0 if np.isnan(recall) else recall.item()\n",
        "  precision = 0 if np.isnan(precision) else precision.item()\n",
        "  F1 = 0 if recall + precision == 0 else  (2*recall*precision/(recall+precision))\n",
        "\n",
        "  print(f\"{recall=}\\n{precision=}\\n{F1=}\\n\")"
      ],
      "metadata": {
        "id": "1gFEpWQ-X7nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 數據裁減\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UsmaGmRwWmxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def argumenting(images, labels, n=5):\n",
        "  \"\"\"\n",
        "  數據增強。\n",
        "  在images裡面把所有有闌尾炎的cut複製給定次數，並\n",
        "  插入回images的隨機位置裡面\n",
        "\n",
        "  Args:\n",
        "    images: 输入的图像数据\n",
        "    labels: 输入的标签数据\n",
        "    n: 每个样本複製多少次\n",
        "\n",
        "  Returns:\n",
        "    增強后的图像数据和标签数据\n",
        "  \"\"\"\n",
        "  where = (labels == True).nonzero(as_tuple=True)[0]\n",
        "  bad_images = images[where]\n",
        "\n",
        "  rep_imgs = bad_images.repeat(n, 1, 1, 1)\n",
        "  rep_labels = torch.ones(rep_imgs.shape[0], dtype=labels.dtype)\n",
        "\n",
        "  # import pdb\n",
        "  # pdb.set_trace()\n",
        "  # 隨機插入argumented圖片\n",
        "  nimg = images.shape[0]\n",
        "  rnd_pos = torch.randint(0, nimg, (rep_imgs.shape[0],))\n",
        "  images = torch.cat((images, rep_imgs), dim=0)\n",
        "  images = images[torch.argsort(torch.cat((torch.arange(nimg), rnd_pos)))]\n",
        "\n",
        "  # 對應對置插入標籤\n",
        "  labels = torch.cat((labels, rep_labels), dim=0)\n",
        "  labels = labels[torch.argsort(torch.cat((torch.arange(nimg), rnd_pos)))]\n",
        "\n",
        "  return images, labels\n",
        "\n",
        "def remove_false_images(images, labels, ratio):\n",
        "    \"\"\"\n",
        "    隨機在images裡面移除一定比例的無闌尾炎cut\n",
        "\n",
        "    Args:\n",
        "      images: 输入的图像数据\n",
        "      labels: 输入的标签数据\n",
        "      ratio: 移除的比例\n",
        "\n",
        "    Returns:\n",
        "      移除后的图像数据和标签数据\n",
        "    \"\"\"\n",
        "    # 找出 labels == 0 的索引\n",
        "    where_false = (labels == 0).nonzero(as_tuple=True)[0]\n",
        "    mask = torch.ones(len(images), dtype=torch.bool)\n",
        "    indices_filter = torch.randperm(len(where_false))[:int(len(where_false)*ratio)]\n",
        "    mask[where_false[indices_filter]] = False\n",
        "    images = images[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def remove_positive_images(images, labels, ratio):\n",
        "    # 找出 labels == 1 的索引\n",
        "    where_positive = (labels == 1).nonzero(as_tuple=True)[0]\n",
        "    mask = torch.ones(len(images), dtype=torch.bool)\n",
        "    indices_filter = torch.randperm(len(where_positive))[:int(len(where_positive)*ratio)]\n",
        "    mask[where_positive[indices_filter]] = False\n",
        "    images = images[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "6nLNS0uRWl5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 其他"
      ],
      "metadata": {
        "id": "BdwzEjFhbR4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dmJYugUB_3w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_zero(images):\n",
        "  count = 0\n",
        "  for image in images:\n",
        "    if not torch.any(image):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def cropping(image):\n",
        "  if image.shape[2] != 512:\n",
        "    start = (image.shape[2] - 512) // 2\n",
        "    end = start + 512\n",
        "    image = image[:, :, start:end, :]\n",
        "\n",
        "  if image.shape[3] != 512:\n",
        "    start = (image.shape[3] - 512) // 2\n",
        "    end = start + 512\n",
        "    image = image[:, :, :, start:end]\n",
        "  return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def custom_sort_key(val):\n",
        "    parts = val.split('_')\n",
        "    if len(parts) == 2 and parts[1].isdigit():\n",
        "        return (parts[0], int(parts[1]))\n",
        "    return (parts[0], -1)  # 使沒有_(數字)的id排最前面\n",
        "\n",
        "\n",
        "def isgpu():\n",
        "    \"\"\"檢查是否有 CUDA 支持的 GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"GPU is available\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        # raise(\"GPU not available\")\n",
        "    return device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud4x-GeICQxw"
      },
      "outputs": [],
      "source": [
        "device = isgpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV66iCZmCrBJ"
      },
      "source": [
        "# 掛載\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKUXzQOdBRFQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "5qbhdYNrGRxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_rootdir = \"/content/drive/MyDrive/AOCR2024\"\n",
        "labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "# datadir =\"Train_Valid_Image\"\n",
        "datadir =\"Cropped_60-316_150-406_11-71/Train_Valid_Image_cropped\"\n"
      ],
      "metadata": {
        "id": "dzzWlsmwlPCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use service account to access googledrive"
      ],
      "metadata": {
        "id": "CwxsNMJG9E-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import googleapiclient.discovery\n",
        "from google.auth.exceptions import TransportError\n",
        "from google.oauth2.service_account import Credentials\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "DriveAPIcredentials = r\"/content/drive/MyDrive/AOCR2024/creds/appendicitis-407217-c2f141040794.json\"\n",
        "credentials = Credentials.from_service_account_file(DriveAPIcredentials,scopes=[\"https://www.googleapis.com/auth/drive\"])\n",
        "drive_service = googleapiclient.discovery.build('drive', 'v3', credentials=credentials)"
      ],
      "metadata": {
        "id": "p46LlGr13crr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 資料下載(非同步)"
      ],
      "metadata": {
        "id": "rnXxst5LARb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.auth.transport.requests import Request\n",
        "request = Request()\n",
        "credentials.refresh(request)\n",
        "loop.run_until_complete(get_data_disk_async(drive_rootdir, datadir,token = credentials.token))"
      ],
      "metadata": {
        "id": "lVtVoGK4AL6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use http request pulic links of google drive files  to download (error 403. be blocked from google)"
      ],
      "metadata": {
        "id": "0e-g7Blb91a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipecho.net/plain"
      ],
      "metadata": {
        "id": "M0o3a7eXiV7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(datadir))"
      ],
      "metadata": {
        "id": "xXH5tN8Jq4Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directly reading files from mounted google drive  \n",
        "**unknown error**: stuck after reading 9 nii files"
      ],
      "metadata": {
        "id": "b8N6QgZI-vWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 測試直接從mount下載\n",
        "# labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "# datadir =\"/content/drive/MyDrive/AOCR2024/Train_Valid_Image\"\n",
        "\n",
        "# process_data2(termi=5,shift=0,dropFalse=0,dropPos=0)"
      ],
      "metadata": {
        "id": "vWZgPEJQ0C9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 測試下載\n",
        "# labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "\n",
        "# dflabel = read_label(labelpath)\n",
        "# images, labels, _ =  process_data2(termi=3,shift=2,dropFalse=0,dropPos=0)"
      ],
      "metadata": {
        "id": "dzXqivsbxDmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s1 = labels[0:60]\n",
        "# s2 = labels[60:120]\n",
        "# s3 = labels[120:180]\n",
        "# s2"
      ],
      "metadata": {
        "id": "m-pRFYuy0xca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset/Model"
      ],
      "metadata": {
        "id": "mei5EcNldUJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, mode='train'):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if self.mode == 'train' or self.mode=='valid':\n",
        "        return self.images[idx], self.labels[idx]\n",
        "      elif self.mode == 'test':\n",
        "        return self.images[idx]"
      ],
      "metadata": {
        "id": "65xrLIRzdMpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"tf_efficientnetv2_m\"\n",
        "pretrained_model = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "# 修改輸入通道\n",
        "pretrained_model.conv_stem = nn.Conv2d(1, 24, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "# 修改輸出類別\n",
        "num_classes = 1\n",
        "pretrained_model.classifier = nn.Linear(pretrained_model.classifier.in_features, num_classes)\n",
        "\n",
        "# 添加 Sigmoid 激活函數\n",
        "pretrained_model = nn.Sequential(\n",
        "    pretrained_model,\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "\n",
        "# 檢查模型結構\n",
        "# print(pretrained_model)\n"
      ],
      "metadata": {
        "id": "jZ0cSVS51BHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loader, model_, optimizer, criterion, device):\n",
        "    model_.train()\n",
        "    running_loss = 0.0\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for batch_images, batch_labels in loader:\n",
        "        batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_(batch_images).squeeze()\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).int()\n",
        "        correct_count += (predicted == batch_labels).sum().item()\n",
        "        total_count += batch_labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_accuracy = correct_count / total_count\n",
        "\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "\n",
        "def valid(loader, model_, criterion, device):\n",
        "  model_.eval()\n",
        "  testing_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  predict_list = torch.tensor([]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch_images, batch_labels in loader:\n",
        "          batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "          outputs = model_(batch_images).squeeze()\n",
        "          predicted = (outputs > 0.5).int()\n",
        "          loss = criterion(outputs, batch_labels)\n",
        "\n",
        "          testing_loss += loss.item()\n",
        "          total += batch_labels.size(0)\n",
        "          correct += (predicted == batch_labels).sum().item()\n",
        "          predict_list = torch.cat((predict_list, predicted), 0)\n",
        "\n",
        "  valid_loss = testing_loss / len(loader)\n",
        "  valid_accuracy = 100 * correct / total\n",
        "\n",
        "  return valid_loss, valid_accuracy, predict_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FA_Fqv_Ex4K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ME8m8YBRFR"
      },
      "source": [
        "# 資料處理"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dflabel = read_label(labelpath)\n",
        "total_nii = len(os.listdir(datadir))\n",
        "images, labels =  process_data2(traindir=datadir, termi=10,shift=0,dropFalse=0,dropPos=0)"
      ],
      "metadata": {
        "id": "WFJpwdc4cSci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images_train, labels_train = remove_false_images(images, labels, 0.9)\n",
        "# print(images_train.shape)\n",
        "# print(len(labels_train))\n",
        "# print(images.shape)"
      ],
      "metadata": {
        "id": "1U8IlMjbME2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(labels_train)"
      ],
      "metadata": {
        "id": "WC2Zjl9fI0pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_train"
      ],
      "metadata": {
        "id": "x9QVRrY2L1VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  %reset_selective images_train, labels_train"
      ],
      "metadata": {
        "id": "jeAD7ynnVOWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset_selective dataset"
      ],
      "metadata": {
        "id": "TExWLDrnQcC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset_selective dataloader"
      ],
      "metadata": {
        "id": "HxHsNkSZQetn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset"
      ],
      "metadata": {
        "id": "ZWaEl6uf0eLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IX5HYZuSKG-"
      },
      "source": [
        "# 訓練 (EfficiencyNetV2_m)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "第一次跑model 跑這\n"
      ],
      "metadata": {
        "id": "0ilQ1VQr3etT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_model\n",
        "model = model.to(device)\n",
        "start = 0"
      ],
      "metadata": {
        "id": "AbYR6JQl3UMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "若是有接續跑模型   \n",
        "跑這epoch要對上"
      ],
      "metadata": {
        "id": "hIdh88yk2Y9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_model\n",
        "model = model.to(device)\n",
        "\n",
        "import json\n",
        "filename = input(\"請輸入要獲取模型路徑:\")\n",
        "\n",
        "if not os.path.isfile(filename+'.pth'):\n",
        "    print(f\"{filename}.pth not exist.\")\n",
        "else:\n",
        "    print(model.load_state_dict(torch.load(filename+'.pth')))\n",
        "\n",
        "if os.path.isfile(filename+'.json'):\n",
        "  with open(f'{filename}.json', 'r') as f:\n",
        "      params = json.load(f)\n",
        "\n",
        "start = int(filename.split('_')[-1]) + 1"
      ],
      "metadata": {
        "id": "PQkT5JF-2JYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 190\n",
        "batch_size = 16\n",
        "lr = 0.01\n",
        "running_loss_list = []\n",
        "modeldirname = input(\"dirname: \")\n",
        "droprate = 0.95\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "images_train, labels_train = remove_false_images(images, labels, droprate)\n",
        "dataset = CustomDataset(images_train, labels_train)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(start, num_epochs):\n",
        "\n",
        "    epoch_loss, epoch_accuracy = train(dataloader, model, optimizer, criterion, device)\n",
        "    running_loss_list.append(epoch_loss)\n",
        "    print(f\"Epoch {epoch}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
        "\n",
        "    images_train, labels_train = remove_false_images(images, labels, droprate)\n",
        "    dataset = CustomDataset(images_train, labels_train)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      savepath = save_model(model, f'{model_name}_{epoch}', dirname=modeldirname)\n",
        "\n",
        "savepath = save_model(model, f'{model_name}_{num_epochs}', dirname=modeldirname)\n"
      ],
      "metadata": {
        "id": "VYZi4cO75LJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhLO9PaUESQh"
      },
      "source": [
        "## Loss圖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvRouRxFEXy2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(1,num_epochs+1)[:len(running_loss_list)], running_loss_list)\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# figname = f'{os.path.dirname(savepath)}/{model_name}_loss.png'\n",
        "# if os.path.isfile(figname):\n",
        "#   print(f\"{figname} exist.\")\n",
        "# else:\n",
        "#   plt.savefig(figname)\n",
        "plt.show()\n",
        "\n",
        "lossname = f'{os.path.dirname(savepath)}/{model_name}_loss.json'\n",
        "\n",
        "with open(f'{lossname}', 'a') as f:\n",
        "  json.dump(running_loss_list, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRrn6mOmBRFU"
      },
      "source": [
        "# 儲存模型參數\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jnh7hW3BRFU"
      },
      "outputs": [],
      "source": [
        "filename = save_model(model)\n",
        "\n",
        "params = {\n",
        "    'num_epochs': num_epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': lr,\n",
        "}\n",
        "\n",
        "if os.path.isfile(filename+'.json'):\n",
        "    print(f\"{filename}.json exist.\")\n",
        "else:\n",
        "    with open(f'{filename}.json', 'w') as f:\n",
        "        json.dump(params, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBcppdev6LlG"
      },
      "source": [
        "# 讀取模型參數"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_model\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Pfx0VwbRN9uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NUlm7dg5sRE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "filename = input(\"請輸入要獲取模型路徑:\")\n",
        "\n",
        "if not os.path.isfile(filename+'.pth'):\n",
        "    print(f\"{filename}.pth not exist.\")\n",
        "else:\n",
        "    print(model.load_state_dict(torch.load(filename+'.pth')))\n",
        "\n",
        "if os.path.isfile(filename+'.json'):\n",
        "  with open(f'{filename}.json', 'r') as f:\n",
        "      params = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v_lVRL47kzj"
      },
      "source": [
        "# 評估\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtWJ-YYy5l2E"
      },
      "outputs": [],
      "source": [
        "# 評估設置\n",
        "try:\n",
        "  batch_size = params['batch_size']\n",
        "except:\n",
        "  batch_size = int(input(\"batch_size:\"))\n",
        "print(batch_size)\n",
        "\n",
        "dataset = CustomDataset(images_train, labels_train)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "valid_loss, valid_accuracy, predict_list = valid(dataloader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the test images: {valid_accuracy}%\")\n",
        "print(f\"Loss: {valid_loss}\")\n",
        "predict_listq = predict_list.cpu()\n"
      ],
      "metadata": {
        "id": "bsCqxcZQ4Mww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy of the network on the 15592   \n",
        "training images: 99.20472036942022%  \n",
        "Loss: 0.03187363331013983\n"
      ],
      "metadata": {
        "id": "fwmwmUjjiYQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s7klq4YR5Qg"
      },
      "outputs": [],
      "source": [
        "print_results(predict_listq, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "H_IU8lUico-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "images讀進來是疊在一起的。 需要info紀錄images檔名順序與分割情況"
      ],
      "metadata": {
        "id": "Z7I_nNXjjlGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdir = \"Cropped_60-316_150-406_11-71/Test_Image_cropped\""
      ],
      "metadata": {
        "id": "kcS21lFCs0bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.auth.transport.requests import Request\n",
        "request = Request()\n",
        "credentials.refresh(request)\n",
        "loop.run_until_complete(get_data_disk_async(drive_rootdir, testdir,token = credentials.token))"
      ],
      "metadata": {
        "id": "6rOoi2_QH-zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_test, info = read_test(testdir,termi=200,shift=0)"
      ],
      "metadata": {
        "id": "tL3mzD9qi-3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_test.shape"
      ],
      "metadata": {
        "id": "fK-YaPTk98bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reset_selective images_test"
      ],
      "metadata": {
        "id": "9y-GcsSABdgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  batch_size = params['batch_size']\n",
        "except:\n",
        "  batch_size = int(input(\"batch_size:\"))\n",
        "print(batch_size)\n",
        "\n",
        "dataset = CustomDataset(images_test, None, mode='test')\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "khCyQWK7d4G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "predict_list = torch.tensor([]).to(device)\n",
        "with torch.no_grad():  # 不更新梯度\n",
        "    for j, batch_images in enumerate(dataloader):\n",
        "        batch_images = batch_images.to(device)\n",
        "        outputs = model(batch_images)\n",
        "        predicted = (outputs.squeeze() > 0.5).int()\n",
        "\n",
        "        # if predicted.dim() == 0:\n",
        "        #   predicted = predicted.unsqueeze(0)\n",
        "        predict_list = torch.cat((predict_list,predicted),0)\n",
        "\n",
        "\n",
        "predict_listq = predict_list.cpu()"
      ],
      "metadata": {
        "id": "N-IzPXwRcrxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_listq.sum()"
      ],
      "metadata": {
        "id": "nzT97RCx9NxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml59tSf2QXty"
      },
      "source": [
        "# 輸出至submission.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練過程不改變資料順序, 所以輸出順序同輸入.   \n",
        "predict值跟隨images順序, 照著info把每個predict值(label)與檔名(id)對上. 輸出成提交格式"
      ],
      "metadata": {
        "id": "4teg7YRZj9Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cont(l):\n",
        "  \"1是否連續\"\n",
        "  f1 = False\n",
        "  f2 = False\n",
        "  for i in range(len(l)):\n",
        "    if (l[i] == 1):\n",
        "      f1 = True\n",
        "    if (l[i] == 0):\n",
        "      if (f1):\n",
        "        f2 = True\n",
        "    if (l[i] == 1) and f2:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def yes(predict):\n",
        "  return int(sum(predict) >= 3)\n",
        "\n",
        "\n",
        "\n",
        "predict_list = predict_listq.numpy()\n",
        "\n",
        "## 還原id與對應label，假設key按照scan輸入順序排列,每個key對應的scan的cuts數是nslice\n",
        "output = {}\n",
        "k = 0  #第幾個scan\n",
        "ii = 0   #每個key輪到第幾個\n",
        "id, nslice = info[k][0], info[k][1]\n",
        "for i in range(len(predict_list)):\n",
        "\n",
        "  if (ii >= nslice):\n",
        "    #該換下一個scan了\n",
        "    output[id] = yes(predict_list[i-nslice:i]) #評估方式\n",
        "\n",
        "    k += 1\n",
        "    ii = 0\n",
        "    id, nslice = info[k][0], info[k][1]\n",
        "\n",
        "  label = predict_list[i]\n",
        "  output[id+f'_{ii}'] = int(predict_list[i])\n",
        "  ii += 1\n",
        "\n",
        "output[id] = yes(predict_list[(i+1)-ii:]) #補上最後一個scan評估\n",
        "# import pdb\n",
        "# pdb.set_trace()\n",
        "output = list(output.items())\n",
        "dfout = pd.DataFrame(output)\n",
        "dfout.columns = ['id', 'label']\n",
        "dfout = dfout.sort_values(by='id', key=lambda x: x.map(custom_sort_key))\n",
        "filename = input(\"輸入提交路徑(default for submission)\")\n",
        "if filename == '':\n",
        "  filename = 'submission'\n",
        "dfout.to_csv(f\"{filename}\"+'.csv', index=False)"
      ],
      "metadata": {
        "id": "eGcPvyce5uDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "id": "T2eaN23dXgfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhDYVsGNxVTZ"
      },
      "outputs": [],
      "source": [
        "dfout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 查看submission.csv"
      ],
      "metadata": {
        "id": "wKe-NfBAeN8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "lC6LJpkZr4xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftest = read_submission('fisrt_80.csv')\n",
        "# dftest = read_submission('submission.csv')\n",
        "dflabel = read_label(\"TrainValid_ground_truth.csv\")"
      ],
      "metadata": {
        "id": "oDM_yM-5bdCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dftest)"
      ],
      "metadata": {
        "id": "fRSaPgTBnbE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# 抓出scan-level data\n",
        "pattern = re.compile(r'.*_[0-9]+$')  # 正則表達式匹配 \"_數字\" 結尾\n",
        "mask = ~dftest.index.str.match(pattern)\n",
        "scan_guess = np.array(dftest[mask]['label'])\n",
        "scan_truth = np.array(dflabel.loc[dftest[mask]['label'].index]['label'])\n",
        "\n",
        "mask = ~mask\n",
        "cut_guess = np.array(dftest[mask]['label'])\n",
        "cut_truth = np.array(dflabel.loc[dftest[mask]['label'].index]['label'])"
      ],
      "metadata": {
        "id": "BD1NfxWTb2iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = 10\n",
        "print(\"=\"*ss + \"F1 score on scan level\" + \"=\"*ss)\n",
        "print_results(scan_guess, scan_truth)\n",
        "print(),print()\n",
        "print(\"=\"*ss + \"F1 score on cut level\" +\"=\"*ss )\n",
        "print_results(cut_guess, cut_truth)"
      ],
      "metadata": {
        "id": "fyMlAtUWuW8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 其他指令\n",
        "不在工作流\n",
        "當參考"
      ],
      "metadata": {
        "id": "FG2tqJ1kMhzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "datadir = 'Train_Valid_Image'\n",
        "for idx, afile in enumerate(os.listdir(datadir)):\n",
        "  file_path = os.path.join(datadir, afile)\n",
        "  time.sleep(1)\n",
        "  nii_file =  nib.load(file_path)\n",
        "  print(f\"{idx}: read {afile}\")"
      ],
      "metadata": {
        "id": "eazF7afOHa8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r'.*_[0-9]+$')  # 正則表達式匹配 \"_數字\" 結尾\n",
        "mask = dftest.index.to_series().str.match(pattern)\n",
        "dflabel_ = dftest[mask]\n",
        "dflabel_.index.map(lambda x : x.split('_')[1]).sort_values()[-3:]"
      ],
      "metadata": {
        "id": "lEForON9S9qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# 假設這是您的列表\n",
        "my_list = np.array([1, 2, 3, 4])\n",
        "\n",
        "mask = [True, False, False, True]\n",
        "my_list[mask]"
      ],
      "metadata": {
        "id": "37C0pWRQTCyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadir =\"TrainValid_Image/train_data\"\n",
        "labelpath = 'TrainValid_ground_truth.csv'\n",
        "labels_ = read_label(labelpath)\n",
        "\n",
        "\n",
        "filelist = os.listdir(datadir)\n",
        "filelist = filelist[10:20]\n",
        "selecteds = [labels_.loc[labels_.index.str.startswith(afile.strip('.nii')+'_')] for afile in filelist]\n",
        "scans_info = []\n",
        "# preallocated\n",
        "print(\"enter\")\n",
        "numcuts = [len(selected) for selected in selecteds]\n",
        "numcut = sum(numcuts)"
      ],
      "metadata": {
        "id": "el4YgUCT6LgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numcuts = [len(selected) for selected in selecteds]"
      ],
      "metadata": {
        "id": "TBvrMIwo6nhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = nib.load('Train_Valid_Image/Zx00AD16F8B97A53DE6E7CFE260BDF122F0E655659A3DF1628.nii.gz')\n"
      ],
      "metadata": {
        "id": "E9rP7qJsIFZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "RFBfoC3iMo1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.randint(3,(5,1,2,2))\n",
        "lab = torch.tensor([1,1,1,1,1])\n",
        "t1"
      ],
      "metadata": {
        "id": "v3E6Q2z4R6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_positive_images(t1,lab,0.79)"
      ],
      "metadata": {
        "id": "vMKO8czxSMd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-cache-dir gdown gdown"
      ],
      "metadata": {
        "id": "-WWE6jOKCD0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "v57WjC4VAtaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/drive/folders/1C7HXpHMw1Alvwif9hO97FUzfn4rhxG8B -O Train_Valid_Image --folder --remaining-ok"
      ],
      "metadata": {
        "id": "1T5_yrOg7RLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RpwX2ZFOBRFT",
        "0IX5HYZuSKG-",
        "rhLO9PaUESQh"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}