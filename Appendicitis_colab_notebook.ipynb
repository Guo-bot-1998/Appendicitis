{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guo-bot-1998/Appendicitis/blob/master/Appendicitis_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coYrDLBnCbwQ"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4kBLfXWRg7N"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n",
        "!pip install tqdm\n",
        "!pip install kora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ4w8h2LkY0g"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import timm\n",
        "import gdown\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import widgets\n",
        "\n",
        "from tqdm.asyncio import tqdm\n",
        "from kora.xattr import get_id\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "loop = asyncio.get_event_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 存取"
      ],
      "metadata": {
        "id": "Oc3DYvY3WsL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_disk(driveroot, targetdir):\n",
        "  \"\"\"從雲端資料夾targetdir下載檔案到colab root底下同名資料夾\n",
        "  只下載沒有下載的檔案.\n",
        "\n",
        "  driveroot為雲端檔案資料夾root\"\"\"\n",
        "\n",
        "  if not os.path.exists('/content/' + targetdir):\n",
        "    os.makedirs(targetdir)\n",
        "    print(f\"making {targetdir} dir...\")\n",
        "\n",
        "  files_on_drive = os.listdir(driveroot+'/'+targetdir)\n",
        "  files_in_localdir = os.listdir(f\"/content/{targetdir}\")\n",
        "  files_to_download = [f for f in files_on_drive if f not in files_in_localdir]\n",
        "  files = [(filename, get_id(f'{driveroot}/{targetdir}/{filename}')) for filename in files_to_download]\n",
        "\n",
        "\n",
        "  for idx, (name, file_id) in enumerate(files):\n",
        "\n",
        "    if not file_id:\n",
        "      print(f\"未找到文件: {name}\")\n",
        "      continue\n",
        "\n",
        "    # 下载文件\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "      status, done = downloader.next_chunk()\n",
        "      # print(f\"下载 {name} 进度: {int(status.progress() * 100)}%\")\n",
        "\n",
        "    # 将文件内容写入本地文件\n",
        "    local_file_path = os.path.join('/content', targetdir, name)\n",
        "\n",
        "    with open(local_file_path, 'wb') as f:\n",
        "      fh.seek(0)\n",
        "      f.write(fh.read())\n",
        "\n",
        "    print(f\"{idx+1}/{len(files)}: {name} downloaded\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "async def download_file(session, name, file_id, file_path, token):\n",
        "  if not file_id:\n",
        "    print(f\"未找到文件: {name}\")\n",
        "    return\n",
        "\n",
        "  download_url = f\"https://www.googleapis.com/drive/v3/files/{file_id}?alt=media\"\n",
        "  headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "  async with session.get(download_url, headers=headers) as response:\n",
        "    if response.status == 200:\n",
        "      with open(file_path, 'wb') as f:\n",
        "        while True:\n",
        "          chunk = await response.content.read(1024)\n",
        "          if not chunk:\n",
        "            break\n",
        "          f.write(chunk)\n",
        "      # print(f\"Downloaded: {name}...\")\n",
        "    else:\n",
        "      print(f\"Failed: {name}!! status: {response.status}\")\n",
        "\n",
        "\n",
        "# 定义异步获取数据的主函数\n",
        "async def get_data_disk_async(driveroot, targetdir, token):\n",
        "  # 生成文件夹的绝对路径\n",
        "  drive_folder_path = os.path.join(driveroot, targetdir)\n",
        "  local_folder_path = os.path.join('/content', targetdir)\n",
        "\n",
        "  if not os.path.exists(local_folder_path):\n",
        "    os.makedirs(local_folder_path)\n",
        "    print(f\"making {targetdir} dir...\")\n",
        "\n",
        "  # 获取文件列表\n",
        "  files_on_drive = os.listdir(drive_folder_path)\n",
        "  files_in_localdir = os.listdir(local_folder_path)\n",
        "  files_to_download = [f for f in files_on_drive if f not in files_in_localdir]\n",
        "  files = [(filename, get_id(os.path.join(drive_folder_path, filename))) for filename in files_to_download]\n",
        "\n",
        "  async with aiohttp.ClientSession() as session:\n",
        "    tasks = []\n",
        "    for idx, (name, file_id) in enumerate(files):\n",
        "      local_file_path = os.path.join(local_folder_path, name)\n",
        "      task = asyncio.create_task(download_file(session, name, file_id, local_file_path, token))\n",
        "      tasks.append(task)\n",
        "\n",
        "    # 使用 tqdm 包装任务列表以显示进度条\n",
        "    for f in tqdm.as_completed(tasks, total=len(tasks)):\n",
        "      await f\n",
        "\n",
        "\n",
        "\n",
        "def process_data2(traindir, shift=0, termi=10, dropFalse=0.85, dropPos=0, files=None):\n",
        "  \"\"\"Reading data in \"traindir\" and process them.\n",
        "  Taking care of memory efficiency\"\n",
        "\n",
        "  Args:\n",
        "    shift: 從第幾個開始讀取 (當files給定時無效)\n",
        "    termi: 讀取幾個 (當files給定時無效)\n",
        "    dropFalse: 被刪除的有闌尾炎的張數比例\n",
        "    dropPos: 被刪除的有闌尾炎的張數比例\n",
        "    files: 要讀取的檔案id，預設是None\n",
        "\n",
        "  Returns:\n",
        "    images: 输出的图像数据\n",
        "    labels: 输出的标签数据\n",
        "  \"\"\"\n",
        "\n",
        "  if not files:\n",
        "    filelist = os.listdir(traindir)\n",
        "    filelist = filelist[shift:shift+termi]\n",
        "  else: filelist = files\n",
        "\n",
        "  return loading_data(traindir, filelist, dropFalse, dropPos)\n",
        "\n",
        "\n",
        "\n",
        "def loading_data(traindir, filelist, dropFalse, dropPos)\\\n",
        " -> tuple[torch.tensor, torch.tensor]:\n",
        "\n",
        "  print(f\"reading {traindir=}...\")\n",
        "  labels_ = read_label(labelpath)\n",
        "\n",
        "  if labels_.index.name != 'id':\n",
        "    labels_.set_index('id', inplace=True)\n",
        "\n",
        "  #裁減過的圖片放在Cropped_[範圍]的資料夾下,用路徑名稱判斷資料使否裁減過\n",
        "  cropmatch = re.search(r'Cropped', traindir)\n",
        "  if not cropmatch:\n",
        "    print(\"讀取未切片的資料夾\")\n",
        "    xlim, ylim, zlim = [0,512], [0,512], [0,None]\n",
        "  else:\n",
        "    #範圍 = xstart-xend_ystart-yend_zstart-zend\n",
        "    ismatch = re.search(r'(\\d+-\\d+)_(\\d+-\\d+)_(\\d+-\\d+)', traindir)\n",
        "    if ismatch:\n",
        "      dimens = ismatch.group(0)\n",
        "      labelidx = dimens.split('_')\n",
        "      xlim = [int(idx) for idx in labelidx[0].split('-')]\n",
        "      ylim = [int(idx) for idx in labelidx[1].split('-')]\n",
        "      zlim = [int(idx) for idx in labelidx[2].split('-')]\n",
        "      print(f\"{xlim=}\\n{ylim=}\\n{zlim=}\")\n",
        "      if xlim[0] >= xlim[1] or ylim[0] >= ylim[1] or zlim[0] >= zlim[1]:\n",
        "        print(\"切片格式不正确\")\n",
        "        return\n",
        "    else:\n",
        "      print(\"切片格式不正确\")\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "  # preallocated(有無裁減過影響preallocated的大小)\n",
        "  if not cropmatch:\n",
        "    selecteds = [labels_.loc[labels_.index.str.startswith(afile.strip('.nii.gz')+'_')] for afile in filelist]\n",
        "    numcuts = [len(selected) for selected in selecteds]\n",
        "    numcuts = sum(numcuts)\n",
        "  else:\n",
        "    numcuts = (zlim[1]-zlim[0]) * len(filelist)\n",
        "  images = torch.zeros(numcuts,1, ylim[-1]-ylim[0],xlim[-1]-xlim[0])\n",
        "  labels = -torch.ones(numcuts)\n",
        "  nimgs = 0\n",
        "  nprocess = 0\n",
        "\n",
        "  for key in filelist:\n",
        "    file_path = os.path.join(traindir, key)\n",
        "    key = key.strip('.nii.gz')\n",
        "    scan  = labels_.loc[labels_.index.str.startswith(key+'_')]\n",
        "    value = nib.load(file_path).get_fdata()\n",
        "\n",
        "    label_t = torch.tensor(scan['label'][zlim[0]:zlim[-1]]) #每個scan只拿zlim範圍，不滿60張的拿完\n",
        "    image_t = torch.from_numpy(value).float().permute(2, 0, 1).unsqueeze(1)\n",
        "\n",
        "    #有幾筆不是512x512, 剪成512x512\n",
        "    if (not cropmatch) and (image_t.shape[2] != 512 or image_t.shape[3] != 512):\n",
        "      image_t = cropping(image_t)\n",
        "\n",
        "    image_t, label_t = remove_false_images(image_t, label_t, dropFalse)\n",
        "    image_t, label_t = remove_positive_images(image_t, label_t, dropPos)\n",
        "\n",
        "    n_new = len(label_t)\n",
        "\n",
        "    images[nimgs:nimgs+n_new] = image_t\n",
        "    labels[nimgs:nimgs+n_new] = label_t\n",
        "    nimgs += len(label_t)\n",
        "    print(f\"Process {nprocess}: {key} finished...\")\n",
        "    nprocess += 1\n",
        "\n",
        "\n",
        "  #after drop\n",
        "  counts = count_zero(images)\n",
        "  images = torch.asarray(images[:len(images)-counts])\n",
        "  labels = labels[labels != -1]\n",
        "\n",
        "  return images, labels.float()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_test(testdir,shift=0,termi=10):\n",
        "  \"\"\"Reading data in \"testdir\"\n",
        "  not memory efficient\n",
        "  Args:\n",
        "    shift: 從第幾個開始讀取\n",
        "    termi: 讀取幾個\n",
        "  Returns:\n",
        "    images: 输出的图像数据\n",
        "    scans_info: 每個scan的檔名和切片數(原本nii檔所包含的數量)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  filelist = os.listdir(testdir)\n",
        "  filelist = filelist[shift:shift+termi]\n",
        "  scans_info = []\n",
        "\n",
        "  images = []\n",
        "  nprocess = 0\n",
        "\n",
        "  for key in filelist:\n",
        "    file_path = os.path.join(testdir, key)\n",
        "    key = key.strip('.nii.gz')\n",
        "    value = nib.load(file_path).get_fdata()\n",
        "    scans_info.append((key, value.shape[2]))\n",
        "\n",
        "    image_t = torch.from_numpy(value).float().permute(2, 0, 1).unsqueeze(1)\n",
        "\n",
        "    images.append(image_t)\n",
        "    print(f\"{nprocess}: Read test {key} finished...\")\n",
        "    nprocess += 1\n",
        "\n",
        "  images = torch.cat(images,dim=0)\n",
        "  print(f\"read {len(scans_info)} scans\")\n",
        "\n",
        "  return images, scans_info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model, modelname=\"\", dirname=\"\", root=os.path.realpath(\"/content/drive/MyDrive/AOCR2024/params\")):\n",
        "\n",
        "  if not model:\n",
        "    print(\"not given model\")\n",
        "    return\n",
        "\n",
        "  if not os.path.exists(root):\n",
        "    print(f\"{root} not exists!\")\n",
        "    return\n",
        "\n",
        "  print(f\"model will be saved as {root}/{dirname}/{modelname}\")\n",
        "\n",
        "  if not modelname:\n",
        "    modelname = input(\"請輸入模型儲存的檔名:\")\n",
        "  if not dirname:\n",
        "    dirname = input(\"請輸入模型儲存的資料夾:\")\n",
        "\n",
        "  filename = f\"{root}/{dirname}/{modelname}\"\n",
        "\n",
        "  if not os.path.exists(os.path.dirname(filename)):\n",
        "    os.mkdir(os.path.dirname(filename))\n",
        "\n",
        "  if os.path.isfile(filename+'.pth'):\n",
        "      print(f\"{filename}.pth exist.\")\n",
        "  else:  #Adam的方向也要存放\n",
        "      torch.save({'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()}, f'{filename}.pth')\n",
        "\n",
        "  return filename\n",
        "\n",
        "\n",
        "\n",
        "def load_model(_model, opti=None, modelname=\"\", dirname=\"\", root=os.path.realpath(\"/content/drive/MyDrive/AOCR2024/params\")):\n",
        "  \"\"\"加載模型參數與優化器，如果沒給優化器，則返回None\n",
        "   訓練參數(_params)同理\n",
        "\n",
        "    return model, opti, params\n",
        "  \"\"\"\n",
        "  if not modelname:\n",
        "    modelname = input(\"請輸入模型參數檔案名稱:\")\n",
        "  if not dirname:\n",
        "    dirname = input(\"請輸入儲存模型的母資料夾名稱:\")\n",
        "\n",
        "  filename = f\"{root}/{dirname}/{modelname}\"\n",
        "  checkpoint = torch.load(filename+'.pth')\n",
        "  _params = None\n",
        "\n",
        "\n",
        "  print(_model.load_state_dict(checkpoint['model_state_dict']))\n",
        "\n",
        "  if os.path.isfile(filename+'.json'):\n",
        "    with open(f'{filename}.json', 'r') as f:\n",
        "      _params = json.load(f)\n",
        "  if opti:\n",
        "    opti.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  return _model,opti,_params, filename\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_label(excel_path) -> pd.DataFrame:\n",
        "  \"\"\"Reads a csv file containing ground-truth.\n",
        "    The csv file should have two columns: 'id' and 'label'.\n",
        "  \"\"\"\n",
        "  with open(excel_path, 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "    df.set_index('id', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def read_split(excel_path):\n",
        "  with open(excel_path, 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "    return df\n",
        "\n",
        "def write_error(logpath, e, nprocess):\n",
        "  # with open(logpath, 'a') as f:\n",
        "    # f.write(f\"{nprocess:} error occured: {e}\\n\") ##! may stuck the program!!\n",
        "  print(f\"{nprocess:} error occured: {e}\\n\")\n",
        "\n",
        "\n",
        "def read_submission(excel_path) -> pd.DataFrame:\n",
        "  \"\"\"Reads a csv file containing submission file.\n",
        "    The csv file should follow the format given by Kaggle.\n",
        "  \"\"\"\n",
        "  with open(excel_path, 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "    df.set_index('id', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KLs_ItZEW0F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 列印結果"
      ],
      "metadata": {
        "id": "Fnggb7x7X8Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion(guess, truth):\n",
        "  if torch.is_tensor(guess):\n",
        "    guess = guess.cpu().numpy()\n",
        "  if torch.is_tensor(truth):\n",
        "    truth = truth.cpu().numpy()\n",
        "\n",
        "  right = guess == truth\n",
        "  wrong = np.logical_not(right)\n",
        "  TP = np.sum(np.logical_and(right, truth == np.ones(right.shape)))\n",
        "  TN = np.sum(np.logical_and(right, truth == np.zeros(right.shape)))\n",
        "  FN = np.sum(np.logical_and(wrong, truth == np.ones(wrong.shape)))\n",
        "  FP = np.sum(np.logical_and(wrong, truth == np.zeros(wrong.shape)))\n",
        "  return (TP,FP,FN,TN)\n",
        "\n",
        "\n",
        "def get_score(TP,FP,FN,TN):\n",
        "  recall = TP/(TP + FN)\n",
        "  precision = TP/(TP + FP)\n",
        "  recall = 0 if np.isnan(recall) else recall.item()\n",
        "  precision = 0 if np.isnan(precision) else precision.item()\n",
        "  F1 = 0 if recall + precision == 0 else  (2*recall*precision/(recall+precision))\n",
        "\n",
        "  return recall, precision, F1\n",
        "\n",
        "\n",
        "def print_results(prediction, labels):\n",
        "  TP,FP,FN,TN = get_confusion(prediction, labels)\n",
        "  recall, precision, F1 = get_score(TP,FP,FN,TN)\n",
        "  print(f\"\\n\\\n",
        "      真實值\\n\\\n",
        "  預  +-----+-----+\\n\\\n",
        "  測| TP: {TP}| FP: {FP}|\\n\\\n",
        "  值| FN: {FN}| TN: {TN}|\\n\\\n",
        "      +-----+-----+ \\n\")\n",
        "\n",
        "  print(f\"{recall=}\\n{precision=}\\n{F1=}\\n\")\n",
        "\n",
        "def create_tab(tabnames):\n",
        "  tb = widgets.TabBar(tabnames)\n",
        "  return tb\n",
        "\n",
        "\n",
        "def display_train(train_info, predict_info):\n",
        "  t_loss, t_accu, v_loss, v_accu = train_info\n",
        "  predict_list, labels_ = predict_info\n",
        "  TP,FP,FN,TN = get_confusion(predict_list, labels_valid[:len(predict_list)])\n",
        "  recall, precision, F1 = get_score(TP,FP,FN,TN)\n",
        "\n",
        "  msg = [f\"Epoch  train_loss  train_accu   valid_loss  valid_accu\",\n",
        "    f\"       recall      precision    F1\"]\n",
        "\n",
        "  print('-'*len(msg[0]))\n",
        "  print(msg[0])\n",
        "  print(f\"{epoch:>5d}: {t_loss:<10.8f}, {t_accu:<10.8f}%, {v_loss:<10.8f}, {v_accu:<10.8f}%\")\n",
        "  print(msg[1])\n",
        "  print(f\"       {recall:<6.8f}, {precision:<11.8f}, {F1:<.8f}\")\n",
        "  print(f\"       {TP=}  {FP=}  {FN=}  {TN=}\")\n",
        "  print('-'*len(msg[0]))"
      ],
      "metadata": {
        "id": "1gFEpWQ-X7nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 數據裁減\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UsmaGmRwWmxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def argumenting(images, labels, n=5):\n",
        "  \"\"\"\n",
        "  數據增強。\n",
        "  在images裡面把所有有闌尾炎的cut複製給定次數，並\n",
        "  插入回images的隨機位置裡面\n",
        "\n",
        "  Args:\n",
        "    images: 输入的图像数据\n",
        "    labels: 输入的标签数据\n",
        "    n: 每个样本複製多少次\n",
        "\n",
        "  Returns:\n",
        "    增強后的图像数据和标签数据\n",
        "  \"\"\"\n",
        "  where = (labels == True).nonzero(as_tuple=True)[0]\n",
        "  bad_images = images[where]\n",
        "\n",
        "  rep_imgs = bad_images.repeat(n, 1, 1, 1)\n",
        "  rep_labels = torch.ones(rep_imgs.shape[0], dtype=labels.dtype)\n",
        "\n",
        "  # import pdb\n",
        "  # pdb.set_trace()\n",
        "  # 隨機插入argumented圖片\n",
        "  nimg = images.shape[0]\n",
        "  rnd_pos = torch.randint(0, nimg, (rep_imgs.shape[0],))\n",
        "  images = torch.cat((images, rep_imgs), dim=0)\n",
        "  images = images[torch.argsort(torch.cat((torch.arange(nimg), rnd_pos)))]\n",
        "\n",
        "  # 對應對置插入標籤\n",
        "  labels = torch.cat((labels, rep_labels), dim=0)\n",
        "  labels = labels[torch.argsort(torch.cat((torch.arange(nimg), rnd_pos)))]\n",
        "\n",
        "  return images, labels\n",
        "\n",
        "def remove_false_images(images, labels, ratio):\n",
        "    \"\"\"\n",
        "    隨機在images裡面移除一定比例的無闌尾炎cut\n",
        "\n",
        "    Args:\n",
        "      images: 输入的图像数据\n",
        "      labels: 输入的标签数据\n",
        "      ratio: 移除的比例\n",
        "\n",
        "    Returns:\n",
        "      移除后的图像数据和标签数据\n",
        "    \"\"\"\n",
        "    # 找出 labels == 0 的索引\n",
        "    where_false = (labels == 0).nonzero(as_tuple=True)[0]\n",
        "    mask = torch.ones(len(images), dtype=torch.bool)\n",
        "    indices_filter = torch.randperm(len(where_false))[:int(len(where_false)*ratio)]\n",
        "    mask[where_false[indices_filter]] = False\n",
        "    images = images[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def remove_positive_images(images, labels, ratio):\n",
        "    # 找出 labels == 1 的索引\n",
        "    where_positive = (labels == 1).nonzero(as_tuple=True)[0]\n",
        "    mask = torch.ones(len(images), dtype=torch.bool)\n",
        "    indices_filter = torch.randperm(len(where_positive))[:int(len(where_positive)*ratio)]\n",
        "    mask[where_positive[indices_filter]] = False\n",
        "    images = images[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def k_fold_split_indices(num_samples, k=5,shuffle_=True):\n",
        "  \"\"\"返回geneartor for index. 省記憶體\"\"\"\n",
        "  state_ = None\n",
        "  if shuffle_:\n",
        "    state_ = 42\n",
        "  kf = KFold(n_splits=k, shuffle=shuffle_, random_state=state_)\n",
        "  return ((train_index, test_index) for train_index, test_index in kf.split(range(num_samples)))\n",
        "\n",
        "\n",
        "splitpath = r\"/content/drive/MyDrive/AOCR2024/TrainValid_split.csv\"\n",
        "def get_split(splitpath=splitpath):\n",
        "  dfsplit = read_split(splitpath)\n",
        "  train_ids,valid_ids = (dfsplit[dfsplit['group']=='Train'])['id'].tolist(),(dfsplit[dfsplit['group']=='Valid'])['id'].tolist()\n",
        "  train_ids,valid_ids = [train_id +'.nii.gz' for train_id in train_ids], [valid_id +'.nii.gz' for valid_id in valid_ids]\n",
        "  return train_ids,valid_ids"
      ],
      "metadata": {
        "id": "6nLNS0uRWl5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 數據處理"
      ],
      "metadata": {
        "id": "bxDFwGkyhKtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 其他"
      ],
      "metadata": {
        "id": "BdwzEjFhbR4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dmJYugUB_3w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_zero(images):\n",
        "  count = 0\n",
        "  for image in images:\n",
        "    if not torch.any(image):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def cropping(image):\n",
        "  if image.shape[2] != 512:\n",
        "    start = (image.shape[2] - 512) // 2\n",
        "    end = start + 512\n",
        "    image = image[:, :, start:end, :]\n",
        "\n",
        "  if image.shape[3] != 512:\n",
        "    start = (image.shape[3] - 512) // 2\n",
        "    end = start + 512\n",
        "    image = image[:, :, :, start:end]\n",
        "  return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def custom_sort_key(val):\n",
        "    parts = val.split('_')\n",
        "    if len(parts) == 2 and parts[1].isdigit():\n",
        "        return (parts[0], int(parts[1]))\n",
        "    return (parts[0], -1)  # 使沒有_(數字)的id排最前面\n",
        "\n",
        "\n",
        "def isgpu():\n",
        "    \"\"\"檢查是否有 CUDA 支持的 GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"GPU is available\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        # raise(\"GPU not available\")\n",
        "    return device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud4x-GeICQxw"
      },
      "outputs": [],
      "source": [
        "device = isgpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV66iCZmCrBJ"
      },
      "source": [
        "# 掛載\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKUXzQOdBRFQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "5qbhdYNrGRxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_rootdir = \"/content/drive/MyDrive/AOCR2024\"\n",
        "labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "# datadir =\"Train_Valid_Image\"\n",
        "datadir =\"Cropped_60-316_150-406_11-71/Train_Valid_Image_cropped\"\n"
      ],
      "metadata": {
        "id": "dzzWlsmwlPCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use service account to access googledrive"
      ],
      "metadata": {
        "id": "CwxsNMJG9E-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import googleapiclient.discovery\n",
        "from google.auth.exceptions import TransportError\n",
        "from google.oauth2.service_account import Credentials\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "DriveAPIcredentials = r\"/content/drive/MyDrive/AOCR2024/creds/appendicitis-407217-c2f141040794.json\"\n",
        "credentials = Credentials.from_service_account_file(DriveAPIcredentials,scopes=[\"https://www.googleapis.com/auth/drive\"])\n",
        "drive_service = googleapiclient.discovery.build('drive', 'v3', credentials=credentials)"
      ],
      "metadata": {
        "id": "p46LlGr13crr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 資料下載(非同步)"
      ],
      "metadata": {
        "id": "rnXxst5LARb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.auth.transport.requests import Request\n",
        "request = Request()\n",
        "credentials.refresh(request)\n",
        "loop.run_until_complete(get_data_disk_async(drive_rootdir, datadir,token = credentials.token))"
      ],
      "metadata": {
        "id": "lVtVoGK4AL6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipecho.net/plain"
      ],
      "metadata": {
        "id": "M0o3a7eXiV7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(datadir))"
      ],
      "metadata": {
        "id": "xXH5tN8Jq4Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset/Model"
      ],
      "metadata": {
        "id": "mei5EcNldUJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, mode='train'):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if self.mode == 'train' or self.mode=='valid':\n",
        "        return self.images[idx], self.labels[idx]\n",
        "      elif self.mode == 'test':\n",
        "        return self.images[idx]"
      ],
      "metadata": {
        "id": "65xrLIRzdMpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tsai's Model"
      ],
      "metadata": {
        "id": "5Q4bQ1pkDyWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class UNetWithDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = DoubleConv(in_channels, 64)\n",
        "        self.down1 = DoubleConv(64, 128)\n",
        "        self.down2 = DoubleConv(128, 256)\n",
        "        self.down3 = DoubleConv(256, 512)\n",
        "        self.down4 = DoubleConv(512, 512)\n",
        "        self.up1 = DoubleConv(1024, 256)\n",
        "        self.up2 = DoubleConv(512, 128)\n",
        "        self.up3 = DoubleConv(256, 64)\n",
        "        self.up4 = DoubleConv(128, 64)\n",
        "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)  # 添加全局平均池化层\n",
        "        self.sigmoid = nn.Sigmoid()  # 添加 Sigmoid 激活函数\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(nn.MaxPool2d(2)(x1))\n",
        "        x3 = self.down2(nn.MaxPool2d(2)(x2))\n",
        "        x4 = self.down3(nn.MaxPool2d(2)(x3))\n",
        "        x5 = self.down4(nn.MaxPool2d(2)(x4))\n",
        "        x = self.up1(torch.cat([nn.Upsample(scale_factor=2)(x5), x4], dim=1))\n",
        "        x = self.up2(torch.cat([nn.Upsample(scale_factor=2)(x), x3], dim=1))\n",
        "        x = self.up3(torch.cat([nn.Upsample(scale_factor=2)(x), x2], dim=1))\n",
        "        x = self.up4(torch.cat([nn.Upsample(scale_factor=2)(x), x1], dim=1))\n",
        "        x = self.outc(x)\n",
        "        x = self.avgpool(x)  # 应用全局平均池化\n",
        "        x = torch.flatten(x, 1)  # 展平\n",
        "        x = self.sigmoid(x)  # 应用 Sigmoid 激活函数\n",
        "        return x\n",
        "\n",
        "\n",
        "## 沒有decoder的\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = DoubleConv(in_channels, 64)\n",
        "        self.down1 = DoubleConv(64, 128)\n",
        "        self.down2 = DoubleConv(128, 256)\n",
        "        self.down3 = DoubleConv(256, 512)\n",
        "        self.down4 = DoubleConv(512, 512)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)  # 添加全局平均池化层\n",
        "        self.fc = nn.Linear(512, out_channels)  # 添加全连接层\n",
        "        self.sigmoid = nn.Sigmoid()  # 添加 Sigmoid 激活函数\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.inc(x)\n",
        "        x = self.down1(nn.MaxPool2d(2)(x))\n",
        "        x = self.down2(nn.MaxPool2d(2)(x))\n",
        "        x = self.down3(nn.MaxPool2d(2)(x))\n",
        "        x = self.down4(nn.MaxPool2d(2)(x))\n",
        "\n",
        "        x = self.avgpool(x)  # 应用全局平均池化\n",
        "        x = torch.flatten(x, 1)  # 展平\n",
        "        x = self.fc(x)  # 应用全连接层\n",
        "        x = self.sigmoid(x)  # 应用 Sigmoid 激活函数\n",
        "        return x\n",
        "\n",
        "# 创建 U-Net 模型实例\n",
        "unet_model = UNet(in_channels=1, out_channels=1)"
      ],
      "metadata": {
        "id": "2OTcf0RADr-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficiencyNetV2_s\n"
      ],
      "metadata": {
        "id": "I7XRbTugDsrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"tf_efficientnetv2_s\"\n",
        "pretrained_model = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "# 修改輸入通道\n",
        "pretrained_model.conv_stem = nn.Conv2d(1, 24, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "# 修改輸出類別\n",
        "num_classes = 1\n",
        "pretrained_model.classifier = nn.Linear(pretrained_model.classifier.in_features, num_classes)\n",
        "\n",
        "# 添加 Sigmoid 激活函數\n",
        "pretrained_model = nn.Sequential(\n",
        "    pretrained_model,\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "\n",
        "# 檢查模型結構\n",
        "# print(pretrained_model)\n"
      ],
      "metadata": {
        "id": "jZ0cSVS51BHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper"
      ],
      "metadata": {
        "id": "Y0FW7UWHjAyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loader, model_, optimizer, criterion, threshold, device):\n",
        "  \"\"\"模型輸出值超過threshold當作陽性\"\"\"\n",
        "  model_.train()\n",
        "  running_loss = 0.0\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "\n",
        "  for batch_images, batch_labels in loader:\n",
        "      batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model_(batch_images).squeeze()\n",
        "      loss = criterion(outputs, batch_labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      predicted = (outputs > threshold).int()\n",
        "      correct_count += (predicted == batch_labels).sum().item()\n",
        "      total_count += batch_labels.size(0)\n",
        "\n",
        "  epoch_loss = running_loss / len(loader)\n",
        "  epoch_accuracy = 100 * correct_count / total_count\n",
        "\n",
        "  return epoch_loss, epoch_accuracy\n",
        "\n",
        "\n",
        "def valid(loader, model_, criterion, threshold, device):\n",
        "  \"\"\"模型輸出值超過threshold當作陽性\"\"\"\n",
        "  model_.eval()\n",
        "  testing_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  predict_list = torch.tensor(()).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for i, (batch_images, batch_labels) in enumerate(loader):\n",
        "          batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "          outputs = model_(batch_images).squeeze()\n",
        "          predicted = (outputs > threshold).int()\n",
        "          loss = criterion(outputs, batch_labels)\n",
        "\n",
        "          testing_loss += loss.item() #單個batch平均loss\n",
        "          bsize = batch_labels.size(0)\n",
        "          total += bsize\n",
        "          correct += (predicted == batch_labels).sum().item()\n",
        "          # import pdb\n",
        "          # pdb.set_trace()\n",
        "          predict_list = torch.concat((predict_list,predicted),0)\n",
        "\n",
        "  valid_loss = testing_loss / len(loader) #單個epoch平均loss\n",
        "  valid_accuracy = 100 * correct / total\n",
        "\n",
        "  return valid_loss, valid_accuracy, predict_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FA_Fqv_Ex4K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ME8m8YBRFR"
      },
      "source": [
        "# 資料處理"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 二八切割"
      ],
      "metadata": {
        "id": "BlxYg5o29uzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_id, valid_id = get_split()\n",
        "images_valid, labels_valid = process_data2(datadir, dropFalse=0, files=valid_id)\n",
        "images, labels = process_data2(datadir, dropFalse=0, files=train_id)"
      ],
      "metadata": {
        "id": "DwIR_e8b8BWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images_valid.shape)\n",
        "print(len(labels_valid))\n",
        "\n",
        "print(images.shape)\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "1srhJYwsDe7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 不割"
      ],
      "metadata": {
        "id": "_uiU4tgDE-Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dflabel = read_label(labelpath)\n",
        "total_nii = len(os.listdir(datadir))\n",
        "images, labels =  process_data2(traindir=datadir, termi=1,shift=0,dropFalse=0,dropPos=0)"
      ],
      "metadata": {
        "id": "WFJpwdc4cSci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "valid當training"
      ],
      "metadata": {
        "id": "Auwx4EE2JuxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_id, valid_id = get_split()\n",
        "images_valid, labels_valid = process_data2(datadir, dropFalse=0, files=valid_id)\n",
        "images, labels = images_valid, labels_valid"
      ],
      "metadata": {
        "id": "ahaOBBHsf6SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check"
      ],
      "metadata": {
        "id": "0WasSlbUEkK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_train, labels_train = remove_false_images(images, labels, 0.90)\n",
        "print(images_train.shape)\n",
        "print(len(labels_train))\n",
        "print(images.shape)\n"
      ],
      "metadata": {
        "id": "1U8IlMjbME2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-fold"
      ],
      "metadata": {
        "id": "9dSulThKilGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "folds_indices = k_fold_split_indices(len(images), k=5, shuffle_=True)\n",
        "# 使用生成的索引\n",
        "for idx, (train_index, test_index) in enumerate(folds_indices):\n",
        "\n",
        "  train_images, test_images = images[train_index], images[test_index]\n",
        "  train_labels, test_labels = labels[train_index], labels[test_index]\n",
        "  print(f\"Fold {idx}: 训练集大小 {train_index}, 测试集大小 {test_index}\")\n",
        "  del train_images, test_images, train_labels, test_labels\n",
        "  gc.collect()\n",
        "#"
      ],
      "metadata": {
        "id": "AcRC4MkKhFf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(labels_train)"
      ],
      "metadata": {
        "id": "WC2Zjl9fI0pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  %reset_selective images_train, labels_train"
      ],
      "metadata": {
        "id": "jeAD7ynnVOWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset"
      ],
      "metadata": {
        "id": "ZWaEl6uf0eLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IX5HYZuSKG-"
      },
      "source": [
        "# 訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 初始化 for first run"
      ],
      "metadata": {
        "id": "Sw6E0WZOELfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficiency\n"
      ],
      "metadata": {
        "id": "0ilQ1VQr3etT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_model\n",
        "model = model.to(device)\n",
        "num_epochs = 400\n",
        "batch_size = 128\n",
        "lr = 0.01\n",
        "droprate = 0.8\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "print(model)\n",
        "\n",
        "start = 0"
      ],
      "metadata": {
        "id": "AbYR6JQl3UMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 初始化 for not first run"
      ],
      "metadata": {
        "id": "hIdh88yk2Y9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_model\n",
        "model = model.to(device)\n",
        "num_epochs = 400\n",
        "lr = 0.01\n",
        "droprate = 0.8\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "model, optimizer, params, filename = load_model(model, optimizer)\n",
        "# print(model)\n",
        "\n",
        "try:\n",
        "  batch_size = params['batch_size']\n",
        "except:\n",
        "  batch_size = int(input(\"batch_size:\"))\n",
        "print(batch_size)\n",
        "\n",
        "start = int(filename.split('_')[-1]) + 1"
      ],
      "metadata": {
        "id": "PQkT5JF-2JYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練"
      ],
      "metadata": {
        "id": "Y6ycO0JKab3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeldirname = input(\"dirname: \")\n",
        "\n",
        "\n",
        "images_train, labels_train = remove_false_images(images, labels, droprate)\n",
        "train_dataset = CustomDataset(images_train, labels_train)\n",
        "valid_dataset = CustomDataset(images_valid, labels_valid)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "running_loss_list = []\n",
        "testing_loss_list = []\n",
        "running_accu_list = []\n",
        "testing_accu_list = []\n",
        "\n",
        "tabnames = [f\"{epoch*10}~{epoch*10+9}\" for epoch in range(num_epochs//10)]\n",
        "tab = create_tab(tabnames)\n",
        "\n",
        "for epoch in range(start, num_epochs):\n",
        "\n",
        "    epoch_loss, epoch_accuracy = train(train_loader, model, optimizer, criterion, 0.5, device)\n",
        "    valid_loss, valid_accuracy, predict_list = valid(valid_loader, model, criterion, 0.5, device)\n",
        "\n",
        "    running_loss_list.append(epoch_loss)\n",
        "    testing_loss_list.append(valid_loss)\n",
        "    running_accu_list.append(epoch_accuracy)\n",
        "    testing_accu_list.append(valid_accuracy)\n",
        "\n",
        "\n",
        "    with tab.output_to(epoch//10):\n",
        "      display_train((epoch_loss, epoch_accuracy, valid_loss, valid_accuracy), (predict_list, labels_valid[:len(predict_list)]))\n",
        "\n",
        "    images_train, labels_train = remove_false_images(images, labels, droprate)\n",
        "    train_dataset = CustomDataset(images_train, labels_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      savepath = save_model(model, f'{model_name}_{epoch}', dirname=modeldirname)\n",
        "\n",
        "savepath = save_model(model, f'{model_name}_{num_epochs}', dirname=modeldirname)\n"
      ],
      "metadata": {
        "id": "VYZi4cO75LJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savepath = save_model(model, f'{model_name}_{epoch}', dirname=modeldirname)"
      ],
      "metadata": {
        "id": "npIxjjyOiD-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhLO9PaUESQh"
      },
      "source": [
        "### Loss圖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvRouRxFEXy2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "\n",
        "n_epochs = len(running_loss_list)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "ax[0].plot(np.arange(start, start+n_epochs,dtype=int), running_loss_list, label='Training Loss')\n",
        "ax[0].plot(np.arange(start, start+n_epochs,dtype=int), testing_loss_list, label='Testing Loss')\n",
        "ax[0].set_title('Loss compare')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend()\n",
        "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))  # 設置 X 軸只顯示整數\n",
        "\n",
        "ax[1].plot(np.arange(start, start+n_epochs,dtype=int), running_accu_list, label='Training Accuracy')\n",
        "ax[1].plot(np.arange(start, start+n_epochs,dtype=int), testing_accu_list, label='Testing Accuracy')\n",
        "ax[1].set_title('Accuracy compare')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))  # 設置 X 軸只顯示整數\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "figname = f'{os.path.dirname(savepath)}/{model_name}_loss.png'\n",
        "if os.path.isfile(figname):\n",
        "  print(f\"{figname} exist.\")\n",
        "else:\n",
        "  plt.savefig(figname)\n",
        "plt.show()\n",
        "\n",
        "lossname = f'{os.path.dirname(savepath)}/{model_name}_loss.json'\n",
        "\n",
        "\n",
        "#若是從checkpoint跑模型,把loss data接上去,loss data預設在模型參數資料夾下\n",
        "if os.path.isfile(lossname):\n",
        "  with open(lossname, 'r') as f:\n",
        "    data_to_save = json.load(f)\n",
        "  data_to_save['running_loss_list'].extend(running_loss_list)\n",
        "  data_to_save['testing_loss_list'].extend(testing_loss_list)\n",
        "  data_to_save['running_accu_list'].extend(running_accu_list)\n",
        "  data_to_save['testing_accu_list'].extend(testing_accu_list)\n",
        "else:\n",
        "  data_to_save = {\n",
        "      'running_loss_list': running_loss_list,\n",
        "      'testing_loss_list': testing_loss_list,\n",
        "      'running_accu_list': running_accu_list,\n",
        "      'testing_accu_list': testing_accu_list\n",
        "  }\n",
        "\n",
        "with open(lossname, 'w') as f:\n",
        "    json.dump(data_to_save, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRrn6mOmBRFU"
      },
      "source": [
        "# 儲存模型參數\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{model_name}_{epoch}')\n",
        "print(f'{modeldirname}')"
      ],
      "metadata": {
        "id": "-m6jzALfUceO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jnh7hW3BRFU"
      },
      "outputs": [],
      "source": [
        "\n",
        "filename = save_model(model)\n",
        "\n",
        "\n",
        "params = {\n",
        "    'num_epochs': epoch,\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': lr,\n",
        "    'droprate': droprate\n",
        "}\n",
        "\n",
        "if os.path.isfile(filename+'.json'):\n",
        "    print(f\"{filename}.json exist.\")\n",
        "else:\n",
        "    with open(f'{filename}.json', 'w') as f:\n",
        "        json.dump(params, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBcppdev6LlG"
      },
      "source": [
        "# 讀取模型參數"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_model\n",
        "model = model.to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "Pfx0VwbRN9uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NUlm7dg5sRE"
      },
      "outputs": [],
      "source": [
        "model,_,params,_ = load_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v_lVRL47kzj"
      },
      "source": [
        "# 單次評估\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtWJ-YYy5l2E"
      },
      "outputs": [],
      "source": [
        "# 評估設置\n",
        "try:\n",
        "  batch_size = params['batch_size']\n",
        "except:\n",
        "  batch_size = int(input(\"batch_size:\"))\n",
        "print(batch_size)\n",
        "\n",
        "dataset = CustomDataset(images_train, labels_train)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "valid_loss, valid_accuracy, predict_list = valid(dataloader, model, criterion, 0.5, device)\n",
        "\n",
        "print(f\"Accuracy of the network on the test images: {valid_accuracy}%\")\n",
        "print(f\"Loss: {valid_loss}\")\n",
        "predict_listq = predict_list.cpu()\n"
      ],
      "metadata": {
        "id": "bsCqxcZQ4Mww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy of the network on the 15592   \n",
        "training images: 99.20472036942022%  \n",
        "Loss: 0.03187363331013983\n"
      ],
      "metadata": {
        "id": "fwmwmUjjiYQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s7klq4YR5Qg"
      },
      "outputs": [],
      "source": [
        "print_results(predict_listq, labels[:len(predict_listq)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 測試"
      ],
      "metadata": {
        "id": "H_IU8lUico-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdir = \"Cropped_60-316_150-406_11-71/Test_Image_cropped\""
      ],
      "metadata": {
        "id": "kcS21lFCs0bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.auth.transport.requests import Request\n",
        "request = Request()\n",
        "credentials.refresh(request)\n",
        "loop.run_until_complete(get_data_disk_async(drive_rootdir, testdir,token = credentials.token))"
      ],
      "metadata": {
        "id": "6rOoi2_QH-zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_test, info = read_test(testdir,termi=200,shift=0)"
      ],
      "metadata": {
        "id": "tL3mzD9qi-3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  batch_size = params['batch_size']\n",
        "except:\n",
        "  batch_size = int(input(\"batch_size:\"))\n",
        "print(batch_size)\n",
        "\n",
        "dataset = CustomDataset(images_test, None, mode='test')\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "khCyQWK7d4G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "predict_list = torch.tensor([]).to(device)\n",
        "with torch.no_grad():  # 不更新梯度\n",
        "    for j, batch_images in enumerate(dataloader):\n",
        "        batch_images = batch_images.to(device)\n",
        "        outputs = model(batch_images)\n",
        "        predicted = (outputs.squeeze() > 0.5).int()\n",
        "\n",
        "        # if predicted.dim() == 0:\n",
        "        #   predicted = predicted.unsqueeze(0)\n",
        "        predict_list = torch.cat((predict_list,predicted),0)\n",
        "\n",
        "\n",
        "predict_listq = predict_list.cpu()"
      ],
      "metadata": {
        "id": "N-IzPXwRcrxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_listq.sum()"
      ],
      "metadata": {
        "id": "nzT97RCx9NxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml59tSf2QXty"
      },
      "source": [
        "# 輸出至提交格式"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "images讀進來是疊在一起的。 需要info紀錄images檔名順序與分割情況"
      ],
      "metadata": {
        "id": "Z7I_nNXjjlGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dflable = read_label(labelpath)"
      ],
      "metadata": {
        "id": "yqYHxF1awyOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r'.*_[0-9]+$')  # 正則表達式匹配 \"_數字\" 結尾\n",
        "mask = dflable.index.str.match(pattern)\n",
        "scan_guess = np.array(dflable[mask]['label'])"
      ],
      "metadata": {
        "id": "Fg4l3kpTxGK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練過程不改變資料順序, 所以輸出順序同輸入.   \n",
        "predict值跟隨images順序, 照著info把每個predict值(label)與檔名(id)對上. 輸出成提交格式"
      ],
      "metadata": {
        "id": "4teg7YRZj9Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yes(predict):\n",
        "  return int(sum(predict) >= 3)\n",
        "\n",
        "\n",
        "\n",
        "predict_list = predict_listq.numpy()\n",
        "\n",
        "## 還原id與對應label，假設key按照scan輸入順序排列,每個key對應的scan的cuts數是nslice\n",
        "output = {}\n",
        "k = 0  #第幾個scan\n",
        "ii = 0   #每個key輪到第幾個\n",
        "id, nslice = info[k][0], info[k][1]\n",
        "for i in range(len(predict_list)):\n",
        "\n",
        "  if (ii >= nslice):\n",
        "    #該換下一個scan了\n",
        "    output[id] = yes(predict_list[i-nslice:i]) #評估方式\n",
        "\n",
        "    k += 1\n",
        "    ii = 0\n",
        "    id, nslice = info[k][0], info[k][1]\n",
        "\n",
        "  label = predict_list[i]\n",
        "  output[id+f'_{ii}'] = int(predict_list[i])\n",
        "  ii += 1\n",
        "\n",
        "output[id] = yes(predict_list[(i+1)-ii:]) #補上最後一個scan評估\n",
        "# import pdb\n",
        "# pdb.set_trace()\n",
        "output = list(output.items())\n",
        "dfout = pd.DataFrame(output)\n",
        "dfout.columns = ['id', 'label']\n",
        "dfout = dfout.sort_values(by='id', key=lambda x: x.map(custom_sort_key))\n",
        "filename = input(\"輸入提交路徑(default for submission)\")\n",
        "if filename == '':\n",
        "  filename = 'submission'\n",
        "dfout.to_csv(f\"{filename}\"+'.csv', index=False)"
      ],
      "metadata": {
        "id": "eGcPvyce5uDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 讀取提交格式的檔案"
      ],
      "metadata": {
        "id": "wKe-NfBAeN8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftest = read_submission('fisrt_80.csv')\n",
        "# dftest = read_submission('submission.csv')\n",
        "dflabel = read_label(\"TrainValid_ground_truth.csv\")"
      ],
      "metadata": {
        "id": "oDM_yM-5bdCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dftest)"
      ],
      "metadata": {
        "id": "fRSaPgTBnbE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# 抓出scan-level data\n",
        "pattern = re.compile(r'.*_[0-9]+$')  # 正則表達式匹配 \"_數字\" 結尾\n",
        "mask = ~dftest.index.str.match(pattern)\n",
        "scan_guess = np.array(dftest[mask]['label'])\n",
        "scan_truth = np.array(dflabel.loc[dftest[mask]['label'].index]['label'])\n",
        "\n",
        "mask = ~mask\n",
        "cut_guess = np.array(dftest[mask]['label'])\n",
        "cut_truth = np.array(dflabel.loc[dftest[mask]['label'].index]['label'])"
      ],
      "metadata": {
        "id": "BD1NfxWTb2iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = 10\n",
        "print(\"=\"*ss + \"F1 score on scan level\" + \"=\"*ss)\n",
        "print_results(scan_guess, scan_truth)\n",
        "print(),print()\n",
        "print(\"=\"*ss + \"F1 score on cut level\" +\"=\"*ss )\n",
        "print_results(cut_guess, cut_truth)"
      ],
      "metadata": {
        "id": "fyMlAtUWuW8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 其他指令\n",
        "不在工作流\n",
        "當參考"
      ],
      "metadata": {
        "id": "FG2tqJ1kMhzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "datadir = 'Train_Valid_Image'\n",
        "for idx, afile in enumerate(os.listdir(datadir)):\n",
        "  file_path = os.path.join(datadir, afile)\n",
        "  time.sleep(1)\n",
        "  nii_file =  nib.load(file_path)\n",
        "  print(f\"{idx}: read {afile}\")"
      ],
      "metadata": {
        "id": "eazF7afOHa8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r'.*_[0-9]+$')  # 正則表達式匹配 \"_數字\" 結尾\n",
        "mask = dftest.index.to_series().str.match(pattern)\n",
        "dflabel_ = dftest[mask]\n",
        "dflabel_.index.map(lambda x : x.split('_')[1]).sort_values()[-3:]"
      ],
      "metadata": {
        "id": "lEForON9S9qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# 假設這是您的列表\n",
        "my_list = np.array([1, 2, 3, 4])\n",
        "\n",
        "mask = [True, False, False, True]\n",
        "my_list[mask]"
      ],
      "metadata": {
        "id": "37C0pWRQTCyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadir =\"TrainValid_Image/train_data\"\n",
        "labelpath = 'TrainValid_ground_truth.csv'\n",
        "labels_ = read_label(labelpath)\n",
        "\n",
        "\n",
        "filelist = os.listdir(datadir)\n",
        "filelist = filelist[10:20]\n",
        "selecteds = [labels_.loc[labels_.index.str.startswith(afile.strip('.nii')+'_')] for afile in filelist]\n",
        "scans_info = []\n",
        "# preallocated\n",
        "print(\"enter\")\n",
        "numcuts = [len(selected) for selected in selecteds]\n",
        "numcut = sum(numcuts)"
      ],
      "metadata": {
        "id": "el4YgUCT6LgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numcuts = [len(selected) for selected in selecteds]"
      ],
      "metadata": {
        "id": "TBvrMIwo6nhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = nib.load('Train_Valid_Image/Zx00AD16F8B97A53DE6E7CFE260BDF122F0E655659A3DF1628.nii.gz')\n"
      ],
      "metadata": {
        "id": "E9rP7qJsIFZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "RFBfoC3iMo1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.randint(3,(5,1,2,2))\n",
        "lab = torch.tensor([1,1,1,1,1])\n",
        "t1"
      ],
      "metadata": {
        "id": "v3E6Q2z4R6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_positive_images(t1,lab,0.79)"
      ],
      "metadata": {
        "id": "vMKO8czxSMd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-cache-dir gdown gdown"
      ],
      "metadata": {
        "id": "-WWE6jOKCD0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "v57WjC4VAtaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/drive/folders/1C7HXpHMw1Alvwif9hO97FUzfn4rhxG8B -O Train_Valid_Image --folder --remaining-ok"
      ],
      "metadata": {
        "id": "1T5_yrOg7RLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "num_epochs = 1\n",
        "batch_size = 16\n",
        "lr = 0.01\n",
        "droprate = 0.8\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "images_train, labels_train = remove_false_images(images, labels, droprate)\n",
        "dataset = CustomDataset(images_train, labels_train)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "for epoch in range(start, num_epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "\n",
        "  for batch_images, batch_labels in dataloader:\n",
        "      batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(batch_images).squeeze()\n",
        "      # loss = criterion(outputs, batch_labels)\n",
        "      # loss.backward()\n",
        "      # optimizer.step()\n",
        "\n",
        "      fig, ax = plt.subplots(1, 2, figsize=(8, 6))\n",
        "\n",
        "      ax[0].imshow(outputs[0].detach().numpy(),cmap='gray')\n",
        "      ax[0].set_title('output')\n",
        "      if batch_labels[0]:\n",
        "        ax[0].text(0.6, 0.7, \"True\", size=10,\n",
        "         ha=\"center\", va=\"center\",\n",
        "         bbox=dict(boxstyle=\"round\",\n",
        "                   ec=(1., 0.5, 0.5),\n",
        "                   fc=(1., 0.8, 0.8),\n",
        "                   )\n",
        "         )\n",
        "      else:\n",
        "        ax[0].text(0.6, 0.7, \"False\", size=10,\n",
        "         ha=\"center\", va=\"center\",\n",
        "         bbox=dict(boxstyle=\"round\",\n",
        "                   ec=(1., 0.5, 0.5),\n",
        "                   fc=(1., 0.8, 0.8),\n",
        "                   )\n",
        "         )\n",
        "      ax[1].imshow(batch_images[0].squeeze().detach().numpy(),cmap='gray')\n",
        "      ax[1].set_title('input')\n",
        "\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "      # running_loss += loss.item()\n",
        "      # predicted = (outputs > 0.5).int()\n",
        "      # correct_count += (predicted == batch_labels).sum().item()\n",
        "      # total_count += batch_labels.size(0)\n",
        "\n",
        "  # epoch_loss = running_loss / len(dataloader)\n",
        "  # epoch_accuracy = correct_count / total_count\n",
        "\n"
      ],
      "metadata": {
        "id": "_q3aCAHFFr95"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RpwX2ZFOBRFT",
        "0IX5HYZuSKG-",
        "rhLO9PaUESQh"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}