{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guo-bot-1998/Appendicitis/blob/master/Appendicitis_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coYrDLBnCbwQ"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q4kBLfXWRg7N",
        "outputId": "34db2adb-475d-4b68-cd63-208856475786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: kora in /usr/local/lib/python3.10/dist-packages (0.9.20)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from kora) (7.34.0)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from kora) (1.5.29)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->kora) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->kora) (23.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->kora) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->kora) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kora) (0.2.12)\n",
            "Requirement already satisfied: fake_useragent in /usr/local/lib/python3.10/dist-packages (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "!pip install tqdm\n",
        "!pip install kora\n",
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bQ4w8h2LkY0g"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import timm\n",
        "import gdown\n",
        "from kora.xattr import get_id\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 存取"
      ],
      "metadata": {
        "id": "Oc3DYvY3WsL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_data2(shift=0, termi=10, dropFalse=0.85, dropPos=0)\\\n",
        " -> tuple[torch.tensor, torch.tensor, list[tuple[str, int]]]:\n",
        "  \"\"\"Reading data in \"datadir\" and process them.\n",
        "  Taking care of memory efficiency\"\n",
        "\n",
        "  Args:\n",
        "    shift: 從第幾個開始讀取\n",
        "    termi: 讀取幾個\n",
        "    dropFalse: 被刪除的有闌尾炎的張數比例\n",
        "    dropPos: 被刪除的有闌尾炎的張數比例\n",
        "\n",
        "  Returns:\n",
        "    images: 输出的图像数据\n",
        "    labels: 输出的标签数据\n",
        "    scans_info: 每個scan的檔名和切片數(原本nii檔所包含的數量)\n",
        "  \"\"\"\n",
        "\n",
        "  labels_ = read_label(labelpath)\n",
        "\n",
        "  if labels_.index.name != 'id':\n",
        "    labels_.set_index('id', inplace=True)\n",
        "\n",
        "  #裁減過的圖片放在Cropped_[範圍]的資料夾下,用路徑名稱判斷資料使否裁減過\n",
        "  cropmatch = re.search(r'Cropped', datadir)\n",
        "  if not cropmatch:\n",
        "    print(\"讀取未切片的資料夾\")\n",
        "    xlim, ylim, zlim = [0,512], [0,512], [0,None]\n",
        "  else:\n",
        "    #範圍 = xstart-xend_ystart-yend_zstart-zend\n",
        "    ismatch = re.search(r'(\\d+-\\d+)_(\\d+-\\d+)_(\\d+-\\d+)', string)\n",
        "    if ismatch:\n",
        "      dimens = ismatch.group(0)\n",
        "      labelidx = dimens.split('_')\n",
        "      xlim = [int(idx) for idx in labelidx[0].split('-')]\n",
        "      ylim = [int(idx) for idx in labelidx[1].split('-')]\n",
        "      zlim = [int(idx) for idx in labelidx[2].split('-')]\n",
        "      print(f\"{xlim=}\\n{ylim=}\\n{zlim=}\")\n",
        "      if xlim[0] >= xlim[1] or ylim[0] >= ylim[1] or zlim[0] >= zlim[1]:\n",
        "        print(\"切片格式不正确\")\n",
        "        return\n",
        "    else:\n",
        "      print(\"切片格式不正确\")\n",
        "      return\n",
        "\n",
        "\n",
        "  filelist = os.listdir(datadir)\n",
        "  filelist = filelist[shift:shift+termi]\n",
        "  scans_info = []\n",
        "\n",
        "  # preallocated\n",
        "  if not cropmatch:\n",
        "    selecteds = [labels_.loc[labels_.index.str.startswith(afile.strip('.nii.gz')+'_')] for afile in filelist]\n",
        "    numcuts = [len(selected) for selected in selecteds]\n",
        "    numcuts = sum(numcuts)\n",
        "  else:\n",
        "    numcuts = (zlim[1]-zlim[0]) * len(filelist)\n",
        "  images = torch.zeros(numcuts,1, ylim[-1]-ylim[0],xlim[-1]-xlim[0])\n",
        "  labels = -torch.ones(numcuts)\n",
        "  nimgs = 0\n",
        "  nprocess = 0\n",
        "\n",
        "  for key in filelist:\n",
        "    file_path = os.path.join(datadir, key)\n",
        "    key = key.strip('.nii.gz')\n",
        "    scan  = labels_.loc[labels_.index.str.startswith(key+'_')]\n",
        "\n",
        "    value = nib.load(file_path).get_fdata()\n",
        "    scans_info.append((key, value.shape[2]))\n",
        "\n",
        "\n",
        "    label_t = torch.tensor(scan['label'][zlim[0]:zlim[-1]])\n",
        "    image_t = torch.from_numpy(value).float().permute(2, 0, 1).unsqueeze(1)\n",
        "\n",
        "    #有幾筆不是512x512\n",
        "    if (not cropmatch) and (image_t.shape[2] != 512 or image_t.shape[3] != 512):\n",
        "      image_t = cropping(image_t)\n",
        "\n",
        "    image_t, label_t = remove_false_images(image_t, label_t, dropFalse)\n",
        "    image_t, label_t = remove_positive_images(image_t, label_t, dropPos)\n",
        "\n",
        "    n_new = len(label_t)\n",
        "\n",
        "    images[nimgs:nimgs+n_new] = image_t\n",
        "    labels[nimgs:nimgs+n_new] = label_t\n",
        "    nimgs += len(label_t)\n",
        "    print(f\"Process {nprocess}: {key} finished...\")\n",
        "    nprocess += 1\n",
        "\n",
        "  print(f\"read {len(scans_info)} scans\")\n",
        "\n",
        "  return images, labels.float(), scans_info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model, modelname=\"\", dirname=\"\", root=os.path.realpath(\"/content/drive/MyDrive/AOCR2024/params\")):\n",
        "\n",
        "  if not model:\n",
        "    print(\"not given model\")\n",
        "    return\n",
        "\n",
        "  if not os.path.exists(root):\n",
        "    print(f\"{root} not exists!\")\n",
        "    return\n",
        "\n",
        "  print(f\"model will be saved under {root}\")\n",
        "\n",
        "\n",
        "  if not modelname:\n",
        "    modelname = input(\"請輸入模型儲存的檔名:\")\n",
        "  if not dirname:\n",
        "    dirname = input(\"請輸入模型儲存的資料夾:\")\n",
        "\n",
        "  filename = f\"{root}/{dirname}/{modelname}\"\n",
        "\n",
        "  if not os.path.exists(os.path.dirname(filename)):\n",
        "    os.mkdir(os.path.dirname(filename))\n",
        "\n",
        "  if os.path.isfile(filename+'.pth'):\n",
        "      print(f\"{filename}.pth exist.\")\n",
        "  else:\n",
        "      torch.save(model.state_dict(), f'{filename}.pth')\n",
        "\n",
        "  return filename\n",
        "\n",
        "\n",
        "\n",
        "def read_label(excel_path) -> pd.DataFrame:\n",
        "  \"\"\"Reads a csv file containing ground-truth.\n",
        "    The csv file should have two columns: 'id' and 'label'.\n",
        "  \"\"\"\n",
        "  with open(excel_path, 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "    df.set_index('id', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def write_error(logpath, e, nprocess):\n",
        "  # with open(logpath, 'a') as f:\n",
        "    # f.write(f\"{nprocess:} error occured: {e}\\n\") ##! may stuck the program!!\n",
        "  print(f\"{nprocess:} error occured: {e}\\n\")\n",
        "\n",
        "\n",
        "def read_submission(excel_path) -> pd.DataFrame:\n",
        "  \"\"\"Reads a csv file containing submission file.\n",
        "    The csv file should follow the format given by Kaggle.\n",
        "  \"\"\"\n",
        "  with open(excel_path, 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "    df.set_index('id', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KLs_ItZEW0F1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 列印結果"
      ],
      "metadata": {
        "id": "Fnggb7x7X8Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion(guess, truth):\n",
        "  right = guess == truth\n",
        "  wrong = np.logical_not(right)\n",
        "  TP = np.sum(np.logical_and(right, truth == np.ones(right.shape)))\n",
        "  TN = np.sum(np.logical_and(right, truth == np.zeros(right.shape)))\n",
        "  FN = np.sum(np.logical_and(wrong, truth == np.ones(wrong.shape)))\n",
        "  FP = np.sum(np.logical_and(wrong, truth == np.zeros(wrong.shape)))\n",
        "  return (TP,FP,FN,TN)\n",
        "\n",
        "\n",
        "\n",
        "def print_results(prediction, labels):\n",
        "  if torch.is_tensor(prediction):\n",
        "    prediction = prediction.cpu().numpy()\n",
        "  if torch.is_tensor(labels):\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "  TP,FP,FN,TN = get_confusion(prediction, labels)\n",
        "  print(f\"\\n\\\n",
        "      真實值\\n\\\n",
        "  預  +-----+-----+\\n\\\n",
        "  測| TP: {TP}| FP: {FP}|\\n\\\n",
        "  值| FN: {FN}| TN: {TN}|\\n\\\n",
        "      +-----+-----+ \\n\")\n",
        "\n",
        "  recall = TP/(TP + FN)\n",
        "  precision = TP/(TP + FP)\n",
        "  recall = 0 if np.isnan(recall) else recall.item()\n",
        "  precision = 0 if np.isnan(precision) else precision.item()\n",
        "  F1 = 0 if recall + precision == 0 else  (2*recall*precision/(recall+precision))\n",
        "\n",
        "  print(f\"{recall=}\\n{precision=}\\n{F1=}\\n\")"
      ],
      "metadata": {
        "id": "1gFEpWQ-X7nc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 數據處理\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UsmaGmRwWmxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def argumenting(images, labels, n=5):\n",
        "  \"\"\"\n",
        "  數據增強。\n",
        "  在images裡面把所有有闌尾炎的cut複製給定次數，並\n",
        "  插入回images的隨機位置裡面\n",
        "\n",
        "  Args:\n",
        "    images: 输入的图像数据\n",
        "    labels: 输入的标签数据\n",
        "    n: 每个样本複製多少次\n",
        "\n",
        "  Returns:\n",
        "    增強后的图像数据和标签数据\n",
        "  \"\"\"\n",
        "  where = (labels == True).nonzero(as_tuple=True)[0]\n",
        "  bad_images = images[where]\n",
        "\n",
        "  rep_imgs = bad_images.repeat(n, 1, 1, 1)\n",
        "  rep_labels = torch.ones(rep_imgs.shape[0], dtype=labels.dtype)\n",
        "\n",
        "  # import pdb\n",
        "  # pdb.set_trace()\n",
        "  # 隨機插入argumented圖片\n",
        "  nimg = images.shape[0]\n",
        "  rnd_pos = torch.randint(0, nimg, (rep_imgs.shape[0],))\n",
        "  images = torch.cat((images, rep_imgs), dim=0)\n",
        "  images = images[torch.argsort(torch.cat((torch.arange(nimg), rnd_pos)))]\n",
        "\n",
        "  # 對應對置插入標籤\n",
        "  labels = torch.cat((labels, rep_labels), dim=0)\n",
        "  labels = labels[torch.argsort(torch.cat((torch.arange(nimg), rnd_pos)))]\n",
        "\n",
        "  return images, labels\n",
        "\n",
        "def remove_false_images(images, labels, ratio):\n",
        "    \"\"\"\n",
        "    隨機在images裡面移除一定比例的無闌尾炎cut\n",
        "\n",
        "    Args:\n",
        "      images: 输入的图像数据\n",
        "      labels: 输入的标签数据\n",
        "      ratio: 移除的比例\n",
        "\n",
        "    Returns:\n",
        "      移除后的图像数据和标签数据\n",
        "    \"\"\"\n",
        "    # 找出 labels == 0 的索引\n",
        "    where_false = (labels == 0).nonzero(as_tuple=True)[0]\n",
        "    mask = torch.ones(len(images), dtype=torch.bool)\n",
        "    indices_filter = torch.randperm(len(where_false))[:int(len(where_false)*ratio)]\n",
        "    mask[where_false[indices_filter]] = False\n",
        "    images = images[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def remove_positive_images(images, labels, ratio):\n",
        "    # 找出 labels == 1 的索引\n",
        "    where_positive = (labels == 1).nonzero(as_tuple=True)[0]\n",
        "    mask = torch.ones(len(images), dtype=torch.bool)\n",
        "    indices_filter = torch.randperm(len(where_positive))[:int(len(where_positive)*ratio)]\n",
        "    mask[where_positive[indices_filter]] = False\n",
        "    images = images[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "6nLNS0uRWl5X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 示例字符串\n",
        "string = \"/content/drive/MyDrive/AOCR2024/Cropped_60-316_150-406_11-71/Train_Valid_Image_cropped/Zx00AD16F8B97A53DE6E7CFE260BDF122F0E655659A3DF1628.nii.gz\"\n",
        "\n",
        "# 正则表达式匹配模式\n",
        "# 这个模式匹配三组“数字-数字”，每组之间由下划线分隔\n",
        "pattern = r'(\\d+-\\d+)_(\\d+-\\d+)_(\\d+-\\d+)'\n",
        "\n",
        "# 使用正则表达式进行匹配\n",
        "match = re.search(pattern, string)\n",
        "\n",
        "if match:\n",
        "    dimens = match.group(0)\n",
        "    labelidx = dimens.split('_')[-1].split('-')\n",
        "    print(\"提取的部分:\", labelidx)\n",
        "else:\n",
        "    print(\"没有找到匹配\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWVFzp79jqJQ",
        "outputId": "d91e676e-d895-4579-8907-f96cefc67d6b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "提取的部分: ['11', '71']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 其他"
      ],
      "metadata": {
        "id": "BdwzEjFhbR4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7dmJYugUB_3w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_zero(images):\n",
        "  count = 0\n",
        "  for image in images:\n",
        "    if not torch.any(image):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def cropping(image):\n",
        "  if image.shape[2] != 512:\n",
        "    start = (image.shape[2] - 512) // 2\n",
        "    end = start + 512\n",
        "    image = image[:, :, start:end, :]\n",
        "\n",
        "  if image.shape[3] != 512:\n",
        "    start = (image.shape[3] - 512) // 2\n",
        "    end = start + 512\n",
        "    image = image[:, :, :, start:end]\n",
        "  return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def custom_sort_key(val):\n",
        "    parts = val.split('_')\n",
        "    if len(parts) == 2 and parts[1].isdigit():\n",
        "        return (parts[0], int(parts[1]))\n",
        "    return (parts[0], -1)  # 使沒有_(數字)的id排最前面\n",
        "\n",
        "\n",
        "def isgpu():\n",
        "    \"\"\"檢查是否有 CUDA 支持的 GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"GPU is available\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        # raise(\"GPU not available\")\n",
        "    return device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ud4x-GeICQxw"
      },
      "outputs": [],
      "source": [
        "device = isgpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV66iCZmCrBJ"
      },
      "source": [
        "# 掛載\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bKUXzQOdBRFQ",
        "outputId": "c937ff47-f27a-42dc-970e-6abe48bcd406",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "5qbhdYNrGRxf",
        "outputId": "0ab7e9f3-f6f0-4659-bc2a-18346ac02f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 資料下載"
      ],
      "metadata": {
        "id": "3OZg-WX2NgQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_rootdir = \"/content/drive/MyDrive/AOCR2024\"\n",
        "labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "\n",
        "datadir =\"Cropped_60-316_150-406_11-71/Train_Valid_Image_cropped\"\n",
        "if not os.path.exists('/content/' +datadir):\n",
        "  os.mkdir(datadir)\n"
      ],
      "metadata": {
        "id": "dzzWlsmwlPCZ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipecho.net/plain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0o3a7eXiV7b",
        "outputId": "7d4a55c5-a9c9-4141-847e-ba4d7d8ae5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.80.136.131"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# user_agents = [\n",
        "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",  # Chrome on Windows 10\n",
        "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",  # Safari on macOS\n",
        "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0\",  # Firefox on Windows 10\n",
        "#     \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\",  # Safari on iPhone\n",
        "#     \"Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.210 Mobile Safari/537.36\",  # Chrome on Android\n",
        "#     \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\"  # Firefox on Ubuntu Linux\n",
        "# ]\n",
        "# headers = {\n",
        "#     'User-Agent': random.choice(user_agents)\n",
        "# }\n",
        "ua = UserAgent()\n",
        "\n",
        "files_on_drive = os.listdir(drive_rootdir+'/'+datadir)\n",
        "files_in_localdir = os.listdir(f\"/content/{datadir}\")\n",
        "files_to_download = [f for f in files_on_drive if f not in files_in_localdir]\n",
        "files = [(filename, get_id(f'{drive_rootdir}/{datadir}/{filename}')) for filename in files_to_download]\n",
        "# print(headers)\n",
        "\n",
        "t = random.randint(20,30)\n",
        "\n",
        "with requests.Session() as session:\n",
        "  for idx, (name, id) in enumerate(files):\n",
        "    headers = {'User-Agent': ua.random}\n",
        "    # print(headers)\n",
        "    url = f\"https://drive.google.com/uc?id={id}&confirm=t\"\n",
        "    # url = f\"https://drive.google.com/file/d/{id}/view?usp=drive_link\"\n",
        "    # url = f\"https://drive.google.com//uc?id={id}&confirm=t&usp=sharing\"\n",
        "    response = session.get(url, headers=headers, allow_redirects=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(f\"/content/{datadir}/{name}\", 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"{idx+1}/{len(files)}: {name}\")\n",
        "    elif response.status_code == 403:\n",
        "        print(f\"Error: Unable to download file. Status code: {response.status_code}\")\n",
        "        # headers = {\n",
        "        #   'User-Agent': random.choice(user_agents)\n",
        "        # }\n",
        "        # print(headers)\n",
        "        s = random.randint(300,600)\n",
        "        print(f\"sleeping {s} secondes...\")\n",
        "        time.sleep(s)\n",
        "    else:\n",
        "        print(f\"Error: Unable to download file. Status code: {response.status_code}\")\n",
        "\n",
        "    # if (idx+1) % t  == 0:\n",
        "    #   s = random.randint(60,300)\n",
        "    #   print(f\"sleeping {s} secondes...\")\n",
        "    #   time.sleep(s)\n",
        "    #   t = random.randint(20,30)\n",
        "\n",
        "# files_on_drive = os.listdir(drive_rootdir+'/'+datadir)\n",
        "# files_in_localdir = os.listdir(f\"/content/{datadir}\")\n",
        "# files_to_download = [f for f in files_on_drive if f not in files_in_localdir]\n",
        "# files = [(filename, get_id(f'{drive_rootdir}/{datadir}/{filename}')) for filename in files_to_download]\n",
        "# for idx, (name, id) in enumerate(files):\n",
        "#   gdown.download(f\"https://drive.google.com/uc?id={id}&confirm=t\", output=f\"/content/{datadir}/{name}\", quiet=True, use_cookies=False)\n",
        "#   print(f\"{idx+1}/{len(files)}: {name}\")"
      ],
      "metadata": {
        "id": "RUZd5b8QlKRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(datadir))"
      ],
      "metadata": {
        "id": "xXH5tN8Jq4Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f668a7b-3dc7-4484-8a69-65b0b13984ac"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 測試直接從mount下載\n",
        "# labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "# datadir =\"/content/drive/MyDrive/AOCR2024/Train_Valid_Image\"\n",
        "\n",
        "# process_data2(termi=5,shift=0,dropFalse=0,dropPos=0)"
      ],
      "metadata": {
        "id": "vWZgPEJQ0C9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 測試下載\n",
        "# labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "\n",
        "# dflabel = read_label(labelpath)\n",
        "# images, labels, _ =  process_data2(termi=3,shift=2,dropFalse=0,dropPos=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzXqivsbxDmZ",
        "outputId": "b9d47b5c-0e87-49dc-aa36-4738833d65d3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xlim=[60, 316]\n",
            "ylim=[150, 406]\n",
            "zlim=[11, 71]\n",
            "Process 0: Zx0AA2C956EEA5EAEA0DC0DA2606A8F8D0F334B97FDD860E1F finished...\n",
            "Process 1: Zx08DA48B45B5C7D0182C369B8E0B013CAF9EEA43199E8AFEF finished...\n",
            "Process 2: Zx0800DE5C96380322E442453B99E25D6F1282F2C4A610A497 finished...\n",
            "read 3 scans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s1 = labels[0:60]\n",
        "# s2 = labels[60:120]\n",
        "# s3 = labels[120:180]\n",
        "# s2"
      ],
      "metadata": {
        "id": "m-pRFYuy0xca"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ME8m8YBRFR"
      },
      "source": [
        "# 資料處理"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labelpath = '/content/drive/MyDrive/AOCR2024/TrainValid_ground_truth.csv'\n",
        "\n",
        "dflabel = read_label(labelpath)\n",
        "total_nii = len(os.listdir(datadir))\n",
        "images, labels, _ =  process_data2(termi=total_nii,shift=0,dropFalse=0,dropPos=0)"
      ],
      "metadata": {
        "id": "WFJpwdc4cSci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd6faed-5cc2-48a0-d8a4-98f17d2416ea"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xlim=[60, 316]\n",
            "ylim=[150, 406]\n",
            "zlim=[11, 71]\n",
            "Process 0: Zx0B06041DB37FAF0160B564B48791D3B58804B731DD57F4AB finished...\n",
            "Process 1: Zx0CA5CE197AA9378EE23F949BDFC91790D4FED5D6FD06F5A2 finished...\n",
            "Process 2: Zx0AA2C956EEA5EAEA0DC0DA2606A8F8D0F334B97FDD860E1F finished...\n",
            "Process 3: Zx08DA48B45B5C7D0182C369B8E0B013CAF9EEA43199E8AFEF finished...\n",
            "Process 4: Zx0800DE5C96380322E442453B99E25D6F1282F2C4A610A497 finished...\n",
            "Process 5: Zx0BCA7F5108A7E8BCC2AB6094DE20A5C243897FCB0C82E20A finished...\n",
            "Process 6: Zx052D253409F40A03B71D313382692559FB09F771A41F754A finished...\n",
            "Process 7: Zx00AD16F8B97A53DE6E7CFE260BDF122F0E655659A3DF1628 finished...\n",
            "Process 8: Zx00FE1B9A88E88C71F8D11F50C2B5FBFEB3461E67BE1E83B8 finished...\n",
            "Process 9: Zx04424F2CF83E0AD3DDF3CC99A41482D909CBC495238F2FEC finished...\n",
            "Process 10: Zx0CE3BA2363A72325A7BB82B688463DBB79D224C67883F062 finished...\n",
            "Process 11: Zx07C5208D04A267A61BFB6EC968FBBB3DF25C245A6893273D finished...\n",
            "Process 12: Zx014E4DF04789E7E61C3B4876CE4D122DC14E0BB842E42E7D finished...\n",
            "Process 13: Zx05576CE976F91BF416232FAF4CDF5F6C5DF852ADCA2B9DD8 finished...\n",
            "Process 14: Zx081F9B5A0C06A23AE10DC1E14E6B9B4DFC71FBCF3B559AEB finished...\n",
            "Process 15: Zx09972B8FA6215EEC75DB4194B16C000F47B5F87AFB64A12B finished...\n",
            "Process 16: Zx0A1564C092259CD377329E07006281E876D09F9B48E77002 finished...\n",
            "Process 17: Zx0800DE5C96380322C65ABA398FC93B5CBDF75B20B11DDF6E finished...\n",
            "Process 18: Zx09D290397499695039422FD2002292BD781E5203436441A3 finished...\n",
            "Process 19: Zx09413D933AD838CE8DB00704AB349855FE4721FC26037F37 finished...\n",
            "Process 20: Zx016CBC4284052F6A74A56DAFFB0B39B95D4DEC066028AA4A finished...\n",
            "Process 21: Zx0C72F2127A469CAA6A49C516DC18B2706A43700721B5D8AB finished...\n",
            "Process 22: Zx04424F2CF83E0AD3059C7706FBE311FDC83AAA8265D2325D finished...\n",
            "Process 23: Zx0BA755CD6D068378215FE1AE4751945053ED5D2F2C2A40E7 finished...\n",
            "Process 24: Zx0CAA076280D8EBEAA52926C4C6F1DB221F5C58F6EC8B2593 finished...\n",
            "Process 25: Zx01F90B532F87127D864B9194B92EDDC3B0EA988ABB5E1D1C finished...\n",
            "Process 26: Zx0A5FF3169135AF89B21F2A97A7278E517EC8B499B0F14C20 finished...\n",
            "Process 27: Zx0CFB1637BD27964BE741B2AA07BC7C84B2B61731E47782E0 finished...\n",
            "Process 28: Zx0AE5424009C101F7422C5BD9DD2C3B0E13E834114A32883A finished...\n",
            "Process 29: Zx0800DE5C96380322A41C31B46264D40D1B72FEC49745785C finished...\n",
            "Process 30: Zx0CFEAEA49D4E19D1219A4C230C6D82751179B29EE9CA9F49 finished...\n",
            "Process 31: Zx09D29039749969505F54A1E8982B5BF5670F5EEABAE98ECF finished...\n",
            "Process 32: Zx018905C639589E81D4C91553041B845F583C3365B076C8E1 finished...\n",
            "Process 33: Zx0534778884B0E2267FCC4CF2BBF361589CD1D845F4A9C9DB finished...\n",
            "Process 34: Zx01F90B532F87127DB647612B6621F2065CA58692B921CEB4 finished...\n",
            "Process 35: Zx015DF8E20804DB94E24A9B2D9DD387A47EF1C620A02026DC finished...\n",
            "Process 36: Zx00FE1B9A88E88C71CF81D0736F23E88BB9C35E2BCDECF501 finished...\n",
            "Process 37: Zx0494BD052F6F8D5A583699BBC329C318AC7B5BE50FD9ACDA finished...\n",
            "Process 38: Zx039B4A139FC862D542D2AACC247C46DFCE9EAA705B5DEF40 finished...\n",
            "Process 39: Zx06DF7CE50F0A454CB7E53B3918C49334BFB49CACC36B7438 finished...\n",
            "read 40 scans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset_selective images"
      ],
      "metadata": {
        "id": "jeAD7ynnVOWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset"
      ],
      "metadata": {
        "id": "ZWaEl6uf0eLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IX5HYZuSKG-"
      },
      "source": [
        "# 訓練 (EfficiencyNetV2_m)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "PjcZY1-Oehib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"tf_efficientnetv2_m\"\n",
        "pretrained_model = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "# 修改輸入通道\n",
        "pretrained_model.conv_stem = nn.Conv2d(1, 24, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "# 修改輸出類別\n",
        "num_classes = 1\n",
        "pretrained_model.classifier = nn.Linear(pretrained_model.classifier.in_features, num_classes)\n",
        "\n",
        "# 添加 Sigmoid 激活函數\n",
        "pretrained_model = nn.Sequential(\n",
        "    pretrained_model,\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# 檢查模型結構\n",
        "# print(pretrained_model)\n"
      ],
      "metadata": {
        "id": "jZ0cSVS51BHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def between_epoch():\n",
        "  del images, labels\n",
        "  images, labels, _ = process_data2(termi=total_nii, shift=0,dropFalse=0.8, dropPos=0)\n",
        "  dataset = CustomDataset(images, labels)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "Ykire39uf_n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdKCQjs_Ozum"
      },
      "outputs": [],
      "source": [
        "# 訓練參數\n",
        "num_epochs = 10\n",
        "batch_size = 8\n",
        "lr = 0.01\n",
        "running_loss_list = []\n",
        "\n",
        "# 初始化模型、損失函數和優化器\n",
        "model = pretrained_model\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "model = model.to(device)\n",
        "\n",
        "dataset = CustomDataset(images, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_images, batch_labels in dataloader:\n",
        "        batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_images)\n",
        "        outputs = outputs.squeeze()\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    dataloader = between_epoch()\n",
        "\n",
        "    running_loss_list.append(running_loss/len(dataloader))\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhLO9PaUESQh"
      },
      "source": [
        "## Loss圖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvRouRxFEXy2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(1,num_epochs+1)[:len(running_loss_list)], running_loss_list)\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRrn6mOmBRFU"
      },
      "source": [
        "# 儲存模型參數\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jnh7hW3BRFU"
      },
      "outputs": [],
      "source": [
        "filename = save_model(model)\n",
        "\n",
        "params = {\n",
        "    'num_epochs': num_epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': lr,\n",
        "}\n",
        "\n",
        "if os.path.isfile(filename+'.json'):\n",
        "    print(f\"{filename}.json exist.\")\n",
        "else:\n",
        "    with open(f'{filename}.json', 'w') as f:\n",
        "        json.dump(params, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBcppdev6LlG"
      },
      "source": [
        "# 讀取模型參數"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = pretrained_model\n",
        "# model = model.to(device)"
      ],
      "metadata": {
        "id": "Pfx0VwbRN9uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NUlm7dg5sRE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "filename = input(\"請輸入要獲取模型路徑:\")\n",
        "\n",
        "if not os.path.isfile(filename+'.pth'):\n",
        "    print(f\"{filename}.pth not exist.\")\n",
        "else:\n",
        "    print(model.load_state_dict(torch.load(filename+'.pth')))\n",
        "\n",
        "with open(f'{filename}.json', 'r') as f:\n",
        "    params = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v_lVRL47kzj"
      },
      "source": [
        "# 評估\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data(\"TrainValid_Image/train_data\",termi=80,shift=80)\n",
        "# data = read_data(\"Test1_Image/test_data\",termi=3,shift=1)\n",
        "dflabel = read_label(\"TrainValid_ground_truth.csv\")"
      ],
      "metadata": {
        "id": "uLNoNdr155p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images,labels,info = process_data(data, dflabel)"
      ],
      "metadata": {
        "id": "4ZfJbfii6FFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtWJ-YYy5l2E"
      },
      "outputs": [],
      "source": [
        "# 評估設置\n",
        "num_epochs = params['num_epochs']\n",
        "batch_size = params['batch_size']\n",
        "\n",
        "#最後data不滿一個batch\n",
        "num_batches = len(images) // batch_size\n",
        "if len(images) % batch_size != 0:\n",
        "  num_batches += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M1qST-FslSs"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "predict_list = torch.tensor([]).to(device)\n",
        "with torch.no_grad():  # 不更新梯度\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(num_batches):\n",
        "        batch_images = images[i*batch_size:(i+1)*batch_size].to(device)\n",
        "        batch_labels = labels[i*batch_size:(i+1)*batch_size].to(device)\n",
        "\n",
        "        outputs = model(batch_images)\n",
        "        predicted = (outputs.squeeze() > 0.5).int()\n",
        "        if predicted.dim() == 0:\n",
        "          predicted = predicted.unsqueeze(0)\n",
        "        predict_list = torch.cat((predict_list,predicted),0)\n",
        "\n",
        "\n",
        "predict_listq = predict_list.cpu()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s7klq4YR5Qg"
      },
      "outputs": [],
      "source": [
        "print_results(predict_listq, labels[:len(predict_list)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml59tSf2QXty"
      },
      "source": [
        "# 輸出至submission.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cont(l):\n",
        "  \"1是否連續\"\n",
        "  f1 = False\n",
        "  f2 = False\n",
        "  for i in range(len(l)):\n",
        "    if (l[i] == 1):\n",
        "      f1 = True\n",
        "    if (l[i] == 0):\n",
        "      if (f1):\n",
        "        f2 = True\n",
        "    if (l[i] == 1) and f2:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def yes(predict):\n",
        "  return int(sum(predict) >= 3)\n",
        "\n",
        "\n",
        "\n",
        "predict_list = predict_listq.numpy()\n",
        "\n",
        "## 還原id與對應label，假設key按照scan輸入順序排列,每個key對應的scan的cuts數是nslice\n",
        "output = {}\n",
        "k = 0  #第幾個scan\n",
        "ii = 0   #每個key輪到第幾個\n",
        "id, nslice = info[k][0], info[k][1]\n",
        "for i in range(len(predict_list)):\n",
        "\n",
        "  if (ii >= nslice):\n",
        "    #該換下一個scan了\n",
        "    output[id] = yes(predict_list[i-nslice:i]) #評估方式\n",
        "\n",
        "    k += 1\n",
        "    ii = 0\n",
        "    id, nslice = info[k][0], info[k][1]\n",
        "\n",
        "  label = predict_list[i]\n",
        "  output[id+f'_{ii}'] = int(predict_list[i])\n",
        "  ii += 1\n",
        "\n",
        "output[id] = yes(predict_list[(i+1)-ii:]) #補上最後一個scan評估\n",
        "# import pdb\n",
        "# pdb.set_trace()\n",
        "output = list(output.items())\n",
        "dfout = pd.DataFrame(output)\n",
        "dfout.columns = ['id', 'label']\n",
        "dfout = dfout.sort_values(by='id', key=lambda x: x.map(custom_sort_key))\n",
        "filename = input(\"輸入提交檔名(enter for submission)\")\n",
        "if filename == '':\n",
        "  filename = 'submission'\n",
        "dfout.to_csv(filename+'.csv', index=False)"
      ],
      "metadata": {
        "id": "eGcPvyce5uDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "id": "T2eaN23dXgfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhDYVsGNxVTZ"
      },
      "outputs": [],
      "source": [
        "dfout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 查看submission.csv"
      ],
      "metadata": {
        "id": "wKe-NfBAeN8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "lC6LJpkZr4xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftest = read_submission('fisrt_80.csv')\n",
        "# dftest = read_submission('submission.csv')\n",
        "dflabel = read_label(\"TrainValid_ground_truth.csv\")"
      ],
      "metadata": {
        "id": "oDM_yM-5bdCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dftest)"
      ],
      "metadata": {
        "id": "fRSaPgTBnbE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# 抓出scan-level data\n",
        "pattern = re.compile(r'.*_[0-9]+$')  # 正則表達式匹配 \"_數字\" 結尾\n",
        "mask = ~dftest.index.str.match(pattern)\n",
        "scan_guess = np.array(dftest[mask]['label'])\n",
        "scan_truth = np.array(dflabel.loc[dftest[mask]['label'].index]['label'])\n",
        "\n",
        "mask = ~mask\n",
        "cut_guess = np.array(dftest[mask]['label'])\n",
        "cut_truth = np.array(dflabel.loc[dftest[mask]['label'].index]['label'])"
      ],
      "metadata": {
        "id": "BD1NfxWTb2iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = 10\n",
        "print(\"=\"*ss + \"F1 score on scan level\" + \"=\"*ss)\n",
        "print_results(scan_guess, scan_truth)\n",
        "print(),print()\n",
        "print(\"=\"*ss + \"F1 score on cut level\" +\"=\"*ss )\n",
        "print_results(cut_guess, cut_truth)"
      ],
      "metadata": {
        "id": "fyMlAtUWuW8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 其他指令\n",
        "不在工作流\n",
        "當參考"
      ],
      "metadata": {
        "id": "FG2tqJ1kMhzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "datadir = 'Train_Valid_Image'\n",
        "for idx, afile in enumerate(os.listdir(datadir)):\n",
        "  file_path = os.path.join(datadir, afile)\n",
        "  time.sleep(1)\n",
        "  nii_file =  nib.load(file_path)\n",
        "  print(f\"{idx}: read {afile}\")"
      ],
      "metadata": {
        "id": "eazF7afOHa8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r'.*_[0-9]+$')  # 正則表達式匹配 \"_數字\" 結尾\n",
        "mask = dftest.index.to_series().str.match(pattern)\n",
        "dflabel_ = dftest[mask]\n",
        "dflabel_.index.map(lambda x : x.split('_')[1]).sort_values()[-3:]"
      ],
      "metadata": {
        "id": "lEForON9S9qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# 假設這是您的列表\n",
        "my_list = np.array([1, 2, 3, 4])\n",
        "\n",
        "mask = [True, False, False, True]\n",
        "my_list[mask]"
      ],
      "metadata": {
        "id": "37C0pWRQTCyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadir =\"TrainValid_Image/train_data\"\n",
        "labelpath = 'TrainValid_ground_truth.csv'\n",
        "labels_ = read_label(labelpath)\n",
        "\n",
        "\n",
        "filelist = os.listdir(datadir)\n",
        "filelist = filelist[10:20]\n",
        "selecteds = [labels_.loc[labels_.index.str.startswith(afile.strip('.nii')+'_')] for afile in filelist]\n",
        "scans_info = []\n",
        "# preallocated\n",
        "print(\"enter\")\n",
        "numcuts = [len(selected) for selected in selecteds]\n",
        "numcut = sum(numcuts)"
      ],
      "metadata": {
        "id": "el4YgUCT6LgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numcuts = [len(selected) for selected in selecteds]"
      ],
      "metadata": {
        "id": "TBvrMIwo6nhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = nib.load('Train_Valid_Image/Zx00AD16F8B97A53DE6E7CFE260BDF122F0E655659A3DF1628.nii.gz')\n"
      ],
      "metadata": {
        "id": "E9rP7qJsIFZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "RFBfoC3iMo1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.randint(3,(5,1,2,2))\n",
        "lab = torch.tensor([1,1,1,1,1])\n",
        "t1"
      ],
      "metadata": {
        "id": "v3E6Q2z4R6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_positive_images(t1,lab,0.79)"
      ],
      "metadata": {
        "id": "vMKO8czxSMd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-cache-dir gdown gdown"
      ],
      "metadata": {
        "id": "-WWE6jOKCD0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "v57WjC4VAtaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/drive/folders/1C7HXpHMw1Alvwif9hO97FUzfn4rhxG8B -O Train_Valid_Image --folder --remaining-ok"
      ],
      "metadata": {
        "id": "1T5_yrOg7RLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RpwX2ZFOBRFT",
        "0IX5HYZuSKG-",
        "rhLO9PaUESQh"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}